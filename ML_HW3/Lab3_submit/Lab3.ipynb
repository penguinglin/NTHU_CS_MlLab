{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "Welcome to your third lab. In this lab, you will learn how to implement linear classifiers with some numerical data (Age, BMI, and Glucose) for predicting Diabetes_mellitus, which means whether the patient has diabetes(1) or not(0).\n",
        "\n",
        "The dataset contains 25000 records for training set and 5000 for testing set.\n",
        "Each instance has 3 features. The features contain Age, BMI, and Glucose.\n",
        "\n",
        "There are three parts in this lab, including\n",
        "\n",
        "  >Part 1: Implement the Perceptron\n",
        "  >\n",
        "  >Part 2: Implement Linear Discriminant Analysis (LDA)\n",
        "  >\n",
        "  >Part 3: Implement Linear Discriminant Analysis (LDA) classifier **using** Gaussian distributions and MAP estimation\n",
        "\n",
        "Please think about the difference between the three classification methods in this lab. Write down your observations in the report.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWd7ExpN6nN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Packages\n",
        "All the packages that you need to finish this assignment are listed below.\n",
        "*   numpy : the fundamental package for scientific computing with Python.\n",
        "*   csv: a built-in Python module to handle CSV files for reading and writing tabular data.\n",
        "*   pandas: a powerful data manipulation and analysis library for structured data, offering DataFrame objects for efficient handling of datasets\n",
        "*   sklearn.metrics.f1_score: calculate the f1_score of the prediction\n",
        "\n",
        "⚠️ **WARNING** ⚠️:\n",
        "*   Please do not import any other packages in this lab.\n",
        "*   np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n",
        "\n",
        "❗ **Important** ❗: Please do not change the code outside this code bracket.\n",
        "```\n",
        "### START CODE HERE ###\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```"
      ],
      "metadata": {
        "id": "qaWGgSrg7D6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages\n",
        "> Note: You **cannot** import any other package in this lab"
      ],
      "metadata": {
        "id": "73G0TlnM8XXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "LrwdtF6S8a91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e949605-c9b5-4938-c0f5-65b82e7552b7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# training_dataroot = '/drive/MyDrive/HW3/lab3_training.csv'\n",
        "# testing_dataroot = '/drive/MyDrive/HW3/lab3_testing.csv'\n",
        "training_dataroot = pd.read_csv('lab3_training.csv')\n",
        "testing_dataroot = pd.read_csv('lab3_testing.csv')\n",
        "\n",
        "# output_path_part1 = '/drive/MyDrive/HW3/lab3_part1.gsheet'\n",
        "# output_path_part2 = '/drive/MyDrive/HW3/lab3_part2.gsheet'\n",
        "# output_path_part3 = '/drive/MyDrive/ml learn/HW3/lab3_part3.gsheet'\n",
        "output_path_part1 = 'lab3_part1.csv'\n",
        "output_path_part2 = 'lab3_part2.csv'\n",
        "output_path_part3 = 'lab3_part3.csv'\n",
        "### END CODE HERE ###\n",
        "\n",
        "# The example data that are used to test the calculation functions\n",
        "X_exp = np.array([[-0.6415175074,\t-0.2499581931,\t-0.8246180885],\n",
        "    [-1.00480699,\t-0.4248545394,\t0.1841706489],\n",
        "    [-0.157131531,\t-0.01062601622,\t0.6666348277],\n",
        "    [-1.125903484,\t2.901126986,\t0.3705772634],\n",
        "    [1.174929904,\t-0.5494975593,\t0.2389961238],\n",
        "    [-0.6415175074,\t-0.1696408072,\t0.6447046377],\n",
        "    [-1.852482448,\t-0.2256164258,\t-0.6820718539],\n",
        "    [-0.5809692604,\t0.5687763118,\t0.4363678332],\n",
        "    [0.5088991865,\t0.866833251,\t-1.021989798],\n",
        "    [1.05383341,\t0.4590381523,\t0.5679489729],\n",
        "    [0.5694474336,\t0.9588184702,\t0.6885650176],\n",
        "    [1.174929904,\t0.09218273166,\t-0.7588275187],\n",
        "    [-0.2176797781,\t-0.7169736941,\t-0.6711067589],\n",
        "    [0.9327369159,\t-1.19479095,\t-0.8026878985],\n",
        "    [-1.368096472,\t0.1146424265,\t-0.4518048595],\n",
        "    [-0.5809692604,\t2.12232714,\t0.3815423584],\n",
        "    [-1.428644719,\t2.01637356,\t-0.627246379],\n",
        "    [0.6905439277,\t-0.5538426799,\t-0.7917228036],\n",
        "    [-1.125903484,\t-1.205814174,\t-0.616281284],\n",
        "    [-0.2782280251,\t-0.6836720793,\t-0.8575133734]])\n",
        "y_exp = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
      ],
      "metadata": {
        "id": "9Ww4EQOSuP-O"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split and preprocess data"
      ],
      "metadata": {
        "id": "6QAiMStrnU5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read input csv to datalist\n",
        "# train = pd.read_csv(training_dataroot)\n",
        "# test = pd.read_csv(testing_dataroot)\n",
        "train = training_dataroot\n",
        "test = testing_dataroot\n",
        "\n",
        "# split data\n",
        "def SplitData(data):\n",
        "\n",
        "  X_train = data.iloc[:20000, :3].values  # Get training features\n",
        "  X_val = data.iloc[20000:, :3].values    # Get validation features\n",
        "  y_train = data.iloc[:20000, 3].values   # Get training labels\n",
        "  y_val = data.iloc[20000:, 3].values      # Get validation labels\n",
        "\n",
        "  return X_train, X_val, y_train, y_val\n",
        "\n",
        "X_train, X_val, y_train, y_val = SplitData(train)\n",
        "X_test = test.iloc[:, :3].values\n",
        "\n",
        "def StandardizeData(X_train, X_val, X_test):\n",
        "  ### START CODE HERE ###\n",
        "    # Calculate mean and standard deviation of the training set\n",
        "    mean_train = X_train.mean(axis=0)\n",
        "    std_train = X_train.std(axis=0)\n",
        "\n",
        "    # Standardize the training set\n",
        "    X_train_standardized = (X_train - mean_train) / std_train\n",
        "\n",
        "    # Standardize validation set\n",
        "    X_val_standardized = (X_val - mean_train) / std_train\n",
        "\n",
        "    # Standardize test set\n",
        "    X_test_standardized = (X_test - mean_train) / std_train\n",
        "\n",
        "    return X_train_standardized, X_val_standardized, X_test_standardized\n",
        "    ### END CODE HERE ###\n",
        "X_train, X_val, X_test = StandardizeData(X_train, X_val, X_test)\n",
        "\n",
        "print(type(X_train), type(X_val), type(X_test)) # <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
        "print(X_train.shape, X_val.shape, X_test.shape) # (20000, 3) (5000, 3) (5000, 3)\n",
        "print(y_train.shape, y_val.shape) # (20000,) (5000,)"
      ],
      "metadata": {
        "id": "TzDmrkBRYeg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b168f6-83c8-4d26-de8b-e76e22153f69"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(20000, 3) (5000, 3) (5000, 3)\n",
            "(20000,) (5000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Perceptron\n",
        "In this part, you'll be implementing key components of a Perceptron. Your task is to complete the linear_combination and predict methods within the Perceptron class.\n",
        "\n",
        "Here's what you need to focus on:\n",
        "\n",
        "  >**The linear_combination function:**\n",
        "  >\n",
        "  >* This function calculates the weighted sum of input features.\n",
        "  >* You'll need to use numpy's dot product function (np.dot) to multiply the input features with their corresponding weights.\n",
        "  >* Remember to add the bias term (w_[0]) to your calculation.\n",
        "  >\n",
        "  >\n",
        "  >**The predict function:**\n",
        "  >\n",
        "  >* This function determines the class prediction by calling linear_combination function.\n",
        "  >* Use numpy's where function to implement the step function: return 1 if the weighted sum is greater than or equal to 0, and 0 otherwise.\n",
        "  >\n",
        "  >\n",
        "  >**The fit function:**\n",
        "  >\n",
        "  >* Iterate through the training data.\n",
        "  >* For each sample, use the predict method (which you've already implemented) to calculate ŷ (y_pred), the predicted value.\n",
        "  >* Updating the weights by the weight update formula (Please note that our labels are 0 and 1, so the formula may be a little bit different with the one in the slides):\n",
        "  >>$W_{i} = W_{i} + Δ(W_{i})$, where $Δ(W_{i}) = (y - \\hat y) \\times X_{i} $\n",
        "  >* Remember to update both the feature weights (w_[1:]) and the bias term (w_[0]).\n",
        "  >* Count the number of misclassifications in each iteration.\n",
        "\n",
        "Hints:\n",
        "\n",
        "* The weights (w_) are stored as a numpy array, with w_[0] as the bias term and w_[1:] as the weights for each feature.\n",
        "* The input X is a 2D numpy array where each row represents a sample and each column a feature.\n",
        "\n",
        "Reference: slides L5 p.8-16\n",
        "\n",
        "**Please save the prediction result in a csv file lab3_part1.csv and upload to Kaggle**"
      ],
      "metadata": {
        "id": "Omdcu9eUSR6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(object):\n",
        "\n",
        "  def __init__(self, X, n_iter=1):\n",
        "    # initializing the weights to 0\n",
        "    self.w_ = np.zeros(1 + X.shape[1])\n",
        "    self.errors_ = []\n",
        "    self.n_iter = n_iter\n",
        "\n",
        "  def linear_combination(self, X):\n",
        "    # calculate the sum of the weighted values\n",
        "    ### START CODE HERE ###\n",
        "    return np.dot(X, self.w_[1:]) + self.w_[0]  # Weighted sum with bias\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    # return the predicted value (ŷ) of X\n",
        "    ### START CODE HERE ###\n",
        "    return np.where(self.linear_combination(X) >= 0, 1, 0)  # Step function\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    print(\"Weights:\", self.w_)\n",
        "\n",
        "    # training the model n_iter times\n",
        "    for _ in range(self.n_iter):\n",
        "      error = 0\n",
        "\n",
        "      # loop through each input\n",
        "      for xi, yi in zip(X, y):\n",
        "        ### START CODE HERE ###\n",
        "        # Calculate ŷ (the predicted value)\n",
        "        y_pred = self.predict(xi.reshape(1, -1))[0]  # Reshape xi for prediction\n",
        "\n",
        "        # Update the weights (note that our labels are 0 and 1)\n",
        "        # Wi = Wi + Δ(Wi) where Δ(Wi) = (y - ŷ) * Xi\n",
        "        update = (yi - y_pred) * xi  # Calculate the update based on prediction error\n",
        "        self.w_[1:] += update  # Update feature weights\n",
        "\n",
        "        # Update the bias term (w_0)\n",
        "        self.w_[0] += (yi - y_pred)  # Update bias\n",
        "\n",
        "        # ŷ != y means mismatches\n",
        "        error += int(y_pred != yi)  # Increment error if prediction is wrong\n",
        "        ### END CODE HERE ###\n",
        "      print(f\"Errors in epoch {_}:\", error)\n",
        "      print(\"Updated Weights:\", self.w_)\n",
        "\n",
        "      self.errors_.append(error)\n",
        "\n",
        "    return self\n"
      ],
      "metadata": {
        "id": "n7FOtfmHhjjh"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the example data to test the weight caculation\n",
        "\n",
        "Expected output:\n",
        "> Weights: [0. 0. 0. 0.]\n",
        ">\n",
        "> Errors in epoch 0: 5\n",
        ">\n",
        "> Updated Weights: [-1.         -2.02260536  2.0821638  -0.75435559]"
      ],
      "metadata": {
        "id": "I0aarnSyZ1rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_exp has been standardized\n",
        "perceptron = Perceptron(X_exp, n_iter=1)\n",
        "perceptron.fit(X_exp, y_exp)"
      ],
      "metadata": {
        "id": "lWY1A2Z3a-fM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a638be0-484f-4d88-9375-1fbe41f6ed4c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [0. 0. 0. 0.]\n",
            "Errors in epoch 0: 5\n",
            "Updated Weights: [-1.         -2.02260536  2.0821638  -0.75435559]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Perceptron at 0x7f08a5116f80>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and validate the model"
      ],
      "metadata": {
        "id": "cVTsGFC4-j5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "# you can change the iteration number if you want\n",
        "perceptron = Perceptron(X_train, n_iter=10)\n",
        "### END CODE HERE ###\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred = perceptron.predict(X_val)\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"\\nF1 Score:\", f1_score(y_val, y_pred))"
      ],
      "metadata": {
        "id": "ShjMSQal2mS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b91ce8-88d9-4c2c-c6ad-2065b5e31aea"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [0. 0. 0. 0.]\n",
            "Errors in epoch 0: 8009\n",
            "Updated Weights: [-1.          0.73444263 -0.04382962  2.26842216]\n",
            "Errors in epoch 1: 8015\n",
            "Updated Weights: [-2.          1.08567614  1.02196519  2.3730904 ]\n",
            "Errors in epoch 2: 8005\n",
            "Updated Weights: [-1.          0.79831081 -0.42299194  1.8048286 ]\n",
            "Errors in epoch 3: 7999\n",
            "Updated Weights: [-2.          0.25538972  1.03835514  1.71244578]\n",
            "Errors in epoch 4: 8046\n",
            "Updated Weights: [0.         0.12773637 0.11695114 1.0385638 ]\n",
            "Errors in epoch 5: 8055\n",
            "Updated Weights: [-1.          0.73444263  0.06070078  1.54970935]\n",
            "Errors in epoch 6: 8056\n",
            "Updated Weights: [-1.          0.67057444 -0.06813246  2.08089048]\n",
            "Errors in epoch 7: 8082\n",
            "Updated Weights: [-1.        -0.5429211  0.0185557  4.2065669]\n",
            "Errors in epoch 8: 8006\n",
            "Updated Weights: [-1.          0.4151017  -0.589156    2.24843147]\n",
            "Errors in epoch 9: 8043\n",
            "Updated Weights: [-2.          0.38312609  0.85241037  1.51920247]\n",
            "\n",
            "Accuracy: 0.5392\n",
            "\n",
            "F1 Score: 0.3022410660205936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the test result"
      ],
      "metadata": {
        "id": "zqU2bONVAAg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = perceptron.predict(X_test)\n",
        "with open(output_path_part1, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['id', 'diabetes_mellitus'])\n",
        "  for i in range(len(y_pred)):\n",
        "    writer.writerow([i, y_pred[i]])"
      ],
      "metadata": {
        "id": "DU2sljAh_wfG"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - LDA\n",
        "\n",
        "In this part, you'll be implementing key components of Linear Discriminant Analysis (LDA).\n",
        "\n",
        "Here's what you need to focus on:\n",
        "\n",
        ">**The fisher_discriminant function**\n",
        ">* Compute the within-class scatter matrix (S_W)\n",
        ">> $S_{W} = \\sum_{n \\in C_{1}}(X_{n}-m_{1})(X_{n}-m_{1})^{T} + \\sum_{n \\in C_{2}}(X_{n}-m_{2})(X_{n}-m_{2})^{T}$\n",
        ">* Compute the between-class scatter matrix (S_B)\n",
        ">> $S_{B} = (m_{2}-m_{1})(m_{2}-m_{1})^{T}$\n",
        ">* Calculate the discriminant vector (w) using S_W and the class means\n",
        ">> $w = S_{W}^{-1}(m_{2}-m_{1})$\n",
        ">>\n",
        ">> note that we define **$m_{1}$=class 0, $m_{2}$=class 1**\n",
        ">* Normalize the discriminant vector\n",
        ">\n",
        ">>Hints:\n",
        ">>* Remember to invert S_W using np.linalg.inv\n",
        ">>* Normalize the final vector using np.linalg.norm\n",
        ">\n",
        ">**The boundary_calculation function**\n",
        ">* This function calculates the decision boundary in the LDA-transformed space\n",
        ">* Calculate the mean of each class in the transformed space\n",
        ">* Compute the decision boundary as the average of these means\n",
        ">\n",
        ">>Hints:\n",
        ">>* Use numpy's mean function (np.mean) with boolean indexing to separate classes\n",
        ">\n",
        ">**The lda_classifier function**\n",
        ">* This function ties everything together to perform LDA classification.\n",
        ">* Project the training and test data onto the LDA space\n",
        ">* Calculate the decision boundary\n",
        ">* Classify the test data based on this boundary\n",
        ">\n",
        ">>Hints:\n",
        ">>* Use the dot product (.dot()) to project data onto the discriminant vector\n",
        ">>* Implement the classification logic using a simple if-else statement\n",
        "\n",
        "**Please save the prediction result in a csv file lab3_part2.csv and upload to Kaggle**\n"
      ],
      "metadata": {
        "id": "emvl27OWCLFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Fisher's linear discriminant function\n",
        "\n",
        "Reference:\n",
        "\n",
        "slides L5 p.22-23\n",
        "\n",
        "https://sthalles.github.io/fisher-linear-discriminant/"
      ],
      "metadata": {
        "id": "fYkZDHgPH9LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fisher_discriminant(X, y):\n",
        "    classes = np.unique(y)\n",
        "\n",
        "    # Compute mean vectors for each class\n",
        "    mean_vectors = [np.mean(X[y == cls], axis=0) for cls in classes]\n",
        "\n",
        "    # Initialize within-class scatter matrix\n",
        "    n_features = X.shape[1]  # Number of features\n",
        "    S_W = np.zeros((n_features, n_features))  # Initialize within-class scatter matrix\n",
        "\n",
        "    for cls, mean_vec in zip(classes, mean_vectors):\n",
        "        class_scatter = np.cov(X[y == cls], rowvar=False)  # Covariance of each class\n",
        "        n_cls = np.sum(y == cls)  # Number of samples in class\n",
        "        S_W += class_scatter * n_cls  # Add class scatter, weighted by class size\n",
        "\n",
        "    # Compute between-class scatter matrix\n",
        "    overall_mean = np.mean(X, axis=0)  # Overall mean of the data\n",
        "    S_B = np.zeros((n_features, n_features))  # Initialize between-class scatter matrix\n",
        "\n",
        "    for cls, mean_vec in zip(classes, mean_vectors):\n",
        "        n_cls = np.sum(y == cls)  # Number of samples in class\n",
        "        mean_diff = (mean_vec - overall_mean).reshape(n_features, 1)  # Difference from overall mean\n",
        "        S_B += n_cls * (mean_diff @ mean_diff.T)  # Add outer product of mean difference, weighted by class size\n",
        "\n",
        "    # Compute the discriminant vector\n",
        "    eig_vals, eig_vecs = np.linalg.eig(np.linalg.pinv(S_W).dot(S_B))  # Use pseudo-inverse to handle singular cases\n",
        "    w = eig_vecs[:, np.argmax(eig_vals)]  # The direction of maximum variance (largest eigenvalue)\n",
        "\n",
        "    # Normalize the discriminant vector\n",
        "    w = w / np.linalg.norm(w)  # Normalize the vector\n",
        "\n",
        "    return w\n"
      ],
      "metadata": {
        "id": "HF6qQtvsGbCC"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the example data to test the weight caculation\n",
        "Expected output:\n",
        "> [ 0.37541286  0.64630924 -0.66434144]"
      ],
      "metadata": {
        "id": "LNbzjNSXMIpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the discriminant\n",
        "# X_exp has been standardized\n",
        "W_exp = fisher_discriminant(X_exp, y_exp)\n",
        "print(W_exp)"
      ],
      "metadata": {
        "id": "I-ldjqUTH3TC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9ac4f9-3c78-452c-aad8-bdc60a119346"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.40287754  0.6584693  -0.63569479]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a classifier"
      ],
      "metadata": {
        "id": "qbL6lBm-TYVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boundary_calculation(X_train_lda, y_train):\n",
        "  # Calculate the means and variances of the classes in the projected space\n",
        "  ### START CODE HERE ###\n",
        "  mean_class_0 = np.mean(X_train_lda[y_train == 0])\n",
        "  mean_class_1 = np.mean(X_train_lda[y_train == 1])\n",
        "\n",
        "  # Calculate the decision boundary (midpoint between the two means)\n",
        "  decision_boundary = (mean_class_0 + mean_class_1) / 2\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return decision_boundary\n",
        "\n",
        "def lda_classifier(X_train, y_train, X_test):\n",
        "\n",
        "  W = fisher_discriminant(X_train, y_train)\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Project onto the first discriminant\n",
        "  X_train_lda = np.dot(X_train, W)  # 将训练集投影到 LDA 判别轴\n",
        "  X_test_lda = np.dot(X_test, W)    # 将测试集投影到 LDA 判别轴\n",
        "\n",
        "  # Calculate decision boundary\n",
        "  decision_boundary = boundary_calculation(X_train_lda, y_train)\n",
        "\n",
        "  # Predict class based on decision boundary\n",
        "  y_pred = np.where(X_test_lda >= decision_boundary, 1, 0)  # 根据决策边界预测类别\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "OPMQBnAuT65h"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the example data to test the boundary calculation\n",
        "Expected output:\n",
        "> 0.5028438197095305"
      ],
      "metadata": {
        "id": "3V2uoF0xewxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "W_exp = fisher_discriminant(X_exp, y_exp)  # 获取判别向量 W\n",
        "X_exp_lda = np.dot(X_exp, W_exp)  # 将训练数据 X_exp 投影到 LDA 判别轴\n",
        "### END CODE HERE ###\n",
        "print(boundary_calculation(X_exp_lda, y_exp))"
      ],
      "metadata": {
        "id": "zwGyWfTze7Dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a197c7cb-8ab5-41b9-a155-28400139b91e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4988826354978516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and validate the model"
      ],
      "metadata": {
        "id": "KyWhnsiUVLvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify the projected test data\n",
        "y_pred = lda_classifier(X_train, y_train, X_val)\n",
        "# print(type(y_pred))\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nF1 Score:\", f1_score(y_val, y_pred))"
      ],
      "metadata": {
        "id": "45jpURFTVK-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701fe5f0-2707-4554-aeb6-1dcf5519f291"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 68.88%\n",
            "\n",
            "F1 Score: 0.6847649918962723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the test result"
      ],
      "metadata": {
        "id": "Wv_daOx5WRho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lda_classifier(X_train, y_train, X_test)\n",
        "with open(output_path_part2, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['id', 'diabetes_mellitus'])\n",
        "  for i in range(len(y_pred)):\n",
        "    writer.writerow([i, y_pred[i]])"
      ],
      "metadata": {
        "id": "YMLVi64KWOrL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 - LDA with MAP\n",
        "\n",
        "In this part, you're implementing a Linear Discriminant Analysis (LDA) classifier **using** Gaussian distributions and Maximum A Posterior (MAP) estimation.\n",
        "\n",
        ">1. **Linear Discriminant Analysis (LDA):**\n",
        ">LDA is a method that finds a linear combination of features that best separates two or more classes. It assumes that the classes are normally distributed with equal covariance matrices.\n",
        ">\n",
        ">2. **Gaussian Density Function:**\n",
        ">The Gaussian (or normal) distribution is defined by its probability density function:\n",
        ">\n",
        ">>$f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$\n",
        ">>\n",
        ">>Where μ is the mean and σ² is the variance.\n",
        ">\n",
        ">3. **Maximum A Posteriori (MAP) Estimation:**\n",
        ">MAP estimation seeks to find the most probable class given the observed data. It combines the likelihood of the data given the class (from the Gaussian density function) with the prior probability of the class.\n",
        "\n",
        "Connecting LDA, Gaussian Distributions, and MAP:\n",
        ">Step 1: LDA projects the data onto a lower-dimensional space that maximizes class separability (the **lda_classifier_map** function in the code)\n",
        ">\n",
        ">Step 2: After projection, we assume each class follows a Gaussian distribution in this new space. Computes the means, variances, and priors of each class in the LDA-projected space. (the **mean_variance_prior** function in the code)\n",
        ">\n",
        ">Step 3: Implement the Gaussian density function. (the **likelihood** function in the code)\n",
        ">\n",
        ">Step 4: Use MAP estimation (the **lda_classifier_map** function in the code)\n",
        ">>* For each test point, calculate its likelihood of belonging to each class using the likelihood function (which you've already implemented).\n",
        ">>* Multiply these likelihoods by the class priors to get quantities proportional to the posterior probabilities.\n",
        ">>* Predict based on the highest posterior probability.\n",
        "\n",
        "**Please save the prediction result in a csv file lab3_part3.csv and upload to Kaggle**\n",
        "\n",
        "Reference: https://sthalles.github.io/fisher-linear-discriminant/\n"
      ],
      "metadata": {
        "id": "y1hhfBbMCs9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a classifier\n",
        "\n",
        "Reference: [Linear Discriminant Analysis](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-lda%E5%88%86%E9%A1%9E%E6%BC%94%E7%AE%97%E6%B3%95-14622f29e4dc)"
      ],
      "metadata": {
        "id": "G8iEGC07ckYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to compute the likelihood\n",
        "def mean_variance_prior(X_train_lda, y_train):\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Calculate the means and variances of the classes in the projected space\n",
        "  mean_class_0 = np.mean(X_train_lda[y_train == 0])\n",
        "  mean_class_1 = np.mean(X_train_lda[y_train == 1])\n",
        "\n",
        "  variance_class_0 = np.var(X_train_lda[y_train == 0])\n",
        "  variance_class_1 = np.var(X_train_lda[y_train == 1])\n",
        "\n",
        "  # 计算先验概率\n",
        "  prior_class_0 = np.mean(y_train == 0)\n",
        "  prior_class_1 = np.mean(y_train == 1)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return mean_class_0, variance_class_0, prior_class_0, mean_class_1, variance_class_1, prior_class_1\n",
        "\n",
        "def likelihood(mean, variance, x): # implement the Gaussian density distribution function\n",
        "  ### START CODE HERE ###\n",
        "  likelihood = (1 / np.sqrt(2 * np.pi * variance)) * np.exp(-0.5 * ((x - mean) ** 2) / variance)\n",
        "  ### END CODE HERE ###\n",
        "  return likelihood\n",
        "\n",
        "def lda_classifier_map(X_train, y_train, X_test):\n",
        "\n",
        "  W = fisher_discriminant(X_train, y_train)\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Project onto the first discriminant\n",
        "  X_train_lda = np.dot(X_train, W).reshape(-1)  # 将 X_train 投影到 LDA 判别轴\n",
        "  X_test_lda = np.dot(X_test, W).reshape(-1)    # 将 X_test 投影到 LDA 判别轴\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  mean_class_0, variance_class_0, prior_class_0, mean_class_1, variance_class_1, prior_class_1 = mean_variance_prior(X_train_lda, y_train)\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Classify based on the maximum posterior probability\n",
        "  predictions = []\n",
        "  for x in X_test_lda:\n",
        "      # 计算后验概率\n",
        "      posterior_class_0 = likelihood(mean_class_0, variance_class_0, x) * prior_class_0\n",
        "      posterior_class_1 = likelihood(mean_class_1, variance_class_1, x) * prior_class_1\n",
        "\n",
        "      # 将样本分配给具有最大后验概率的类别\n",
        "      if posterior_class_0 > posterior_class_1:\n",
        "          predictions.append(0)\n",
        "      else:\n",
        "          predictions.append(1)\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return np.array(predictions)"
      ],
      "metadata": {
        "id": "B5wgzjcIcrqq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the example data to test the likelihood calculation\n",
        "If the differences between your output and expected output are only in last few decimal places, it's unlikely to affect the model's final results.\n",
        "\n",
        "Expected output:\n",
        ">means: 0.029797169482780564 0.9762686734207664\n",
        ">\n",
        ">variances: 0.36972519622481526 0.22878455291253338\n",
        ">\n",
        ">priors: 0.8421052631578947 0.15789473684210525\n",
        ">\n",
        ">likelihoods: 0.6560640840455648 0.11464725416998729"
      ],
      "metadata": {
        "id": "ANsnJ280m_ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_class_0, variance_class_0, prior_class_0, mean_class_1, variance_class_1, prior_class_1 = mean_variance_prior(X_exp_lda[:19], y_exp[:19])\n",
        "print(\"means:\", mean_class_0, mean_class_1)\n",
        "print(\"variances:\", variance_class_0, variance_class_1)\n",
        "print(\"priors:\", prior_class_0, prior_class_1)\n",
        "print(\"likelihoods:\", likelihood(mean_class_0, variance_class_0, X_exp_lda[19]), likelihood(mean_class_1, variance_class_1, X_exp_lda[19]))"
      ],
      "metadata": {
        "id": "ak9M6fWAnCBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1922ee47-fd62-41e8-9424-83877dc055ee"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "means: 0.019184256397725086 0.9807184481733625\n",
            "variances: 0.3908276254241098 0.20163768973624152\n",
            "priors: 0.8421052631578947 0.15789473684210525\n",
            "likelihoods: 0.637065154158319 0.07521212637861983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and validate the model"
      ],
      "metadata": {
        "id": "6cD0iDKEcmIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify the projected test data\n",
        "y_pred = lda_classifier_map(X_train, y_train, X_val)\n",
        "# print(type(y_pred))\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nF1 Score:\", f1_score(y_val, y_pred))"
      ],
      "metadata": {
        "id": "TOBXCOrlcqil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2369d0d4-a948-4a17-f32a-983f08f8a3d1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 68.82%\n",
            "\n",
            "F1 Score: 0.6875125275606334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the test result"
      ],
      "metadata": {
        "id": "IfArDC9qhxms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = lda_classifier_map(X_train, y_train, X_test)\n",
        "# Write the prediction to output csv\n",
        "with open(output_path_part3, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['id', 'diabetes_mellitus'])\n",
        "  for i in range(len(y_pred)):\n",
        "    writer.writerow([i, y_pred[i]])"
      ],
      "metadata": {
        "id": "HTb1pDJVhzOJ"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}