{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab1: Regression**\n",
        "In *lab 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement the gradient descent regression model to predict people's grip force from their weight.\n",
        "\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implementing a regression model to predict grip force in a different way (for example, with more variables) than the basic part\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aMASY5gD9L0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1. Basic Part (50%)\n",
        "In the first part, you need to implement the regression to predict grip force\n",
        "\n",
        "Please save the prediction result in a CSV file and submit it to Kaggle"
      ],
      "metadata": {
        "id": "yNpd_FfX_BXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Packages\n",
        "\n",
        "> Note: You **cannot** import any other package\n"
      ],
      "metadata": {
        "id": "egBMMLGV_X_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhhUTua487C-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global attributes\n",
        "Define the global attributes\\\n",
        "You can also add your own global attributes here"
      ],
      "metadata": {
        "id": "8iuXHvhLALwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataroot = 'lab1_basic_training.csv' # Training data file file named as 'lab1_basic_training.csv'\n",
        "testing_dataroot = 'lab1_basic_testing.csv'   # Testing data file named as 'lab1_basic_testing.csv'\n",
        "output_dataroot = 'lab1_basic.csv' # Output file will be named as 'lab1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be a list with 100 elements"
      ],
      "metadata": {
        "id": "wXZVhdp8-flF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Input File\n",
        "First, load the basic input file **lab1_basic_training.csv** and **lab1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ],
      "metadata": {
        "id": "IyTqIRxQAtWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = pd.read_csv(testing_dataroot).to_numpy()"
      ],
      "metadata": {
        "id": "KUzYjoq9AwRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the Regression Model\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions"
      ],
      "metadata": {
        "id": "QFXG-axpAcom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n"
      ],
      "metadata": {
        "id": "9bqYH_MvBv4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SplitData(data, split_ratio):\n",
        "    \"\"\"\n",
        "    Splits the given dataset into training and validation sets based on the specified split ratio.\n",
        "\n",
        "    Parameters:\n",
        "    - data (numpy.ndarray): The dataset to be split. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
        "    - split_ratio (float): The ratio of the data to be used for training. For example, a value of 0.8 means 80% of the data will be used for training and the remaining 20% for validation.\n",
        "\n",
        "    Returns:\n",
        "    - training_data (numpy.ndarray): The portion of the dataset used for training.\n",
        "    - validation_data (numpy.ndarray): The portion of the dataset used for validation.\n",
        "\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "    validation_data = []\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    return training_data, validation_data\n",
        "\n"
      ],
      "metadata": {
        "id": "6K2QUnt-A-1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle unreasonable data and missing data\n",
        "\n",
        "> Hint 1: Outliers and missing data can be addressed by either removing them or replacing them using statistical methods (e.g., the mean of all data).\n",
        "\n",
        "> Hint 2: Missing data are represented as `np.nan`, so functions like `np.isnan()` can be used to detect them.\n",
        "\n",
        "> Hint 3: Methods such as the Interquartile Range (IQR) can help detect outliers"
      ],
      "metadata": {
        "id": "-miSLyewCeME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PreprocessData(data):\n",
        "    \"\"\"\n",
        "    Preprocess the given dataset and return the result.\n",
        "\n",
        "    Parameters:\n",
        "    - data (numpy.ndarray): The dataset to preprocess. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
        "\n",
        "    Returns:\n",
        "    - preprocessedData (numpy.ndarray): Preprocessed data.\n",
        "    \"\"\"\n",
        "    preprocessedData = []\n",
        "\n",
        "    # TODO\n",
        "\n",
        "\n",
        "\n",
        "    return preprocessedData\n"
      ],
      "metadata": {
        "id": "jR4TYnwwCrci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Implement Regression\n",
        "You have to use Gradient Descent to finish this part"
      ],
      "metadata": {
        "id": "csS9lL8DCzZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Regression(dataset):\n",
        "    \"\"\"\n",
        "    Performs regression on the given dataset and return the coefficients.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset (numpy.ndarray): A 2D array where each row represents a data point.\n",
        "\n",
        "    Returns:\n",
        "    - w (numpy.ndarray): The coefficients of the regression model. For example, y = w[0] + w[1] * x + w[2] * x^2 + ...\n",
        "    \"\"\"\n",
        "\n",
        "    X = dataset[:, :1]\n",
        "    y = dataset[:, 1]\n",
        "\n",
        "    # TODO: Decide on the degree of the polynomial\n",
        "    degree = 2  # For example, quadratic regression\n",
        "\n",
        "    # Add polynomial features to X\n",
        "    X_poly = np.ones((X.shape[0], 1))  # Add intercept term (column of ones)\n",
        "    for d in range(1, degree + 1):\n",
        "        X_poly = np.hstack((X_poly, X ** d))  # Add x^d terms to feature matrix\n",
        "\n",
        "    # Initialize coefficients (weights) to zero\n",
        "    num_dimensions = X_poly.shape[1]  # Number of features (including intercept and polynomial terms)\n",
        "    w = np.zeros(num_dimensions)\n",
        "\n",
        "    # TODO: Set hyperparameters\n",
        "    num_iteration = 1000\n",
        "    learning_rate = 0.01\n",
        "\n",
        "\n",
        "    # Gradient Descent\n",
        "    m = len(y)  # Number of data points\n",
        "    for iteration in range(num_iteration):\n",
        "        # TODO: Prediction using current weights and compute error\n",
        "\n",
        "\n",
        "        # TODO: Compute gradient\n",
        "\n",
        "\n",
        "        # TODO: Update the weights\n",
        "\n",
        "\n",
        "        # TODO: Optionally, print the cost every 100 iterations\n",
        "        if iteration % 100 == 0:\n",
        "            cost = ...\n",
        "            print(f\"Iteration {iteration}, Cost: {cost}\")\n",
        "\n",
        "    return w\n"
      ],
      "metadata": {
        "id": "n8ftprTRC0Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*"
      ],
      "metadata": {
        "id": "inqQ4lh8DFY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def MakePrediction(w, test_dataset):\n",
        "    \"\"\"\n",
        "    Predicts the output for a given test dataset using a regression model.\n",
        "\n",
        "    Parameters:\n",
        "    - w (numpy.ndarray): The coefficients of the model, where each element corresponds to\n",
        "                               a coefficient for the respective power of the independent variable.\n",
        "    - test_dataset (numpy.ndarray): A 1D array containing the input values (independent variable)\n",
        "                                          for which predictions are to be made.\n",
        "\n",
        "    Returns:\n",
        "    - list/numpy.ndarray: A list or 1d array of predicted values corresponding to each input value in the test dataset.\n",
        "    \"\"\"\n",
        "    prediction = []\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "WwGE2qjgDLwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Train Model and Generate Result\n",
        "\n",
        "Use the above functions to train your model on training dataset, and predict the answer of testing dataset.\n",
        "\n",
        "Save your predicted values in `output_datalist`\n",
        "\n",
        "> Notice: **Remember to inclue the coefficients of your model in the report**\n",
        "\n"
      ],
      "metadata": {
        "id": "-q4qKXbDDmG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "# (1) Split data\n",
        "\n",
        "# (2) Preprocess data\n",
        "\n",
        "# (3) Train regression model\n",
        "\n",
        "# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n",
        "\n",
        "# (5) Make prediction of testing dataset and store the values in output_datalist\n"
      ],
      "metadata": {
        "id": "coo82WvZDpMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv and upload the file to Kaggle\n",
        "> Format: 'Id', 'gripForce'\n"
      ],
      "metadata": {
        "id": "RW3NrFmGEEiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume that output_datalist is a list (or 1d array) with length = 100\n",
        "\n",
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['Id', 'gripForce'])\n",
        "  for i in range(len(output_datalist)):\n",
        "    writer.writerow([i,output_datalist[i]])\n"
      ],
      "metadata": {
        "id": "Mo7rdhx0EFLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Code_final version"
      ],
      "metadata": {
        "id": "twwn4rGtx2Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient descent"
      ],
      "metadata": {
        "id": "v5jvUB2szSai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 檔案路徑\n",
        "training_dataroot = 'lab1_basic_training.csv'  # 訓練資料檔案\n",
        "testing_dataroot = 'lab1_basic_testing.csv'    # 測試資料檔案\n",
        "output_dataroot = 'lab1_basic.csv'             # 預測結果輸出檔案\n",
        "\n",
        "# 讀取CSV檔案\n",
        "training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
        "testing_datalist = pd.read_csv(testing_dataroot).to_numpy()\n",
        "\n",
        "def SplitData(data, split_ratio):\n",
        "    \"\"\"\n",
        "    根據指定的比例將資料集分割為訓練集和驗證集。\n",
        "\n",
        "    參數:\n",
        "    - data (numpy.ndarray): 要分割的資料集，每行代表一個數據點。\n",
        "    - split_ratio (float): 用於訓練的資料比例（例如，0.8表示80%的資料用於訓練）。\n",
        "\n",
        "    回傳:\n",
        "    - training_data (numpy.ndarray): 訓練集。\n",
        "    - validation_data (numpy.ndarray): 驗證集。\n",
        "    \"\"\"\n",
        "    num_training_samples = int(len(data) * split_ratio)\n",
        "    training_data = data[:num_training_samples]\n",
        "    validation_data = data[num_training_samples:]\n",
        "    return training_data, validation_data\n",
        "\n",
        "def PreprocessData(data):\n",
        "    \"\"\"\n",
        "    預處理資料集，刪除包含遺失值或極端值的整行資料。\n",
        "\n",
        "    參數:\n",
        "    - data (numpy.ndarray): 要預處理的資料集，每行代表一個數據點。\n",
        "\n",
        "    回傳:\n",
        "    - preprocessedData (numpy.ndarray): 預處理後的資料集。\n",
        "    \"\"\"\n",
        "    # 將資料轉換為 DataFrame 便於處理\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 步驟 1: 刪除包含遺失值的行\n",
        "    initial_shape = df.shape\n",
        "    df.dropna(inplace=True)\n",
        "    after_dropna_shape = df.shape\n",
        "    print(f\"Deleted {initial_shape[0] - after_dropna_shape[0]} rows containing missing values.\")\n",
        "\n",
        "    # 步驟 2: 刪除包含極端值的行\n",
        "    outlier_indices = set()\n",
        "    for col in df.columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        # 找出該欄位的極端值索引\n",
        "        col_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
        "        outlier_indices.update(col_outliers)\n",
        "        print(f\"Column {col}: Q1 = {Q1:.2f}, Q3 = {Q3:.2f}, IQR = {IQR:.2f}\")\n",
        "        print(f\"Outlier range for column {col}: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "        print(f\"Number of outliers in column {col}: {len(col_outliers)}\")\n",
        "\n",
        "    # 刪除所有包含任何極端值的行\n",
        "    df_before_outlier_drop = df.shape\n",
        "    df.drop(index=outlier_indices, inplace=True)\n",
        "    df_after_outlier_drop = df.shape\n",
        "    print(f\"Deleted {df_before_outlier_drop[0] - df_after_outlier_drop[0]} rows containing outliers.\")\n",
        "\n",
        "    # 最終預處理後的資料\n",
        "    preprocessedData = df.to_numpy()\n",
        "\n",
        "    print(f\"Preprocessed data shape: {preprocessedData.shape}\")\n",
        "\n",
        "    return preprocessedData\n",
        "\n",
        "def compute_gradient(w, b, x, y):\n",
        "    \"\"\"\n",
        "    計算成本函數對權重和偏差的梯度。\n",
        "\n",
        "    參數:\n",
        "    - w (float): 當前權重。\n",
        "    - b (float): 當前偏差。\n",
        "    - x (numpy.ndarray): 輸入特徵。\n",
        "    - y (numpy.ndarray): 真實目標值。\n",
        "\n",
        "    回傳:\n",
        "    - w_gradient (float): 對權重的梯度。\n",
        "    - b_gradient (float): 對偏差的梯度。\n",
        "    \"\"\"\n",
        "    w_gradient = (x * (w * x + b - y)).mean()\n",
        "    b_gradient = ((w * x + b - y)).mean()\n",
        "    return w_gradient, b_gradient\n",
        "\n",
        "def compute_cost(x, y, w, b):\n",
        "    \"\"\"\n",
        "    計算均方誤差成本。\n",
        "\n",
        "    參數:\n",
        "    - x (numpy.ndarray): 輸入特徵。\n",
        "    - y (numpy.ndarray): 真實目標值。\n",
        "    - w (float): 權重。\n",
        "    - b (float): 偏差。\n",
        "\n",
        "    回傳:\n",
        "    - cost (float): 均方誤差成本。\n",
        "    \"\"\"\n",
        "    y_pred = x * w + b\n",
        "    cost = ((y - y_pred) ** 2).mean()\n",
        "    return cost\n",
        "\n",
        "def Regression(dataset):\n",
        "    \"\"\"\n",
        "    使用梯度下降進行線性迴歸。\n",
        "\n",
        "    參數:\n",
        "    - dataset (numpy.ndarray): 資料集，每行代表一個數據點。\n",
        "\n",
        "    回傳:\n",
        "    - w (float): 最終權重。\n",
        "    - b (float): 最終偏差。\n",
        "    - cost_hist (list): 成本歷史。\n",
        "    - w_hist (list): 權重歷史。\n",
        "    - b_hist (list): 偏差歷史。\n",
        "    \"\"\"\n",
        "    x = dataset[:, :-1].flatten()  # 輸入特徵\n",
        "    y = dataset[:, -1]             # 目標值\n",
        "\n",
        "    # 超參數\n",
        "    w = -10\n",
        "    b = 0\n",
        "    num_iteration = 70000\n",
        "    learning_rate_w = 0.000102  # 權重的學習率\n",
        "    learning_rate_b = 0.000201   # 偏差的學習率\n",
        "\n",
        "    cost_hist = []\n",
        "    w_hist = []\n",
        "    b_hist = []\n",
        "\n",
        "    for iteration in range(num_iteration):\n",
        "        w_gradient, b_gradient = compute_gradient(w, b, x, y)\n",
        "        w -= w_gradient * learning_rate_w  # 使用權重的學習率更新 w\n",
        "        b -= b_gradient * learning_rate_b  # 使用偏差的學習率更新 b\n",
        "        cost = compute_cost(x, y, w, b)\n",
        "\n",
        "        w_hist.append(w)\n",
        "        b_hist.append(b)\n",
        "        cost_hist.append(cost)\n",
        "\n",
        "        if iteration % 200 == 0 or iteration == num_iteration - 1:\n",
        "            print(f\"w = {w:.5f}, b = {b:.5f}\")\n",
        "            print(f\"w_gradient = {w_gradient:.5f}, b_gradient = {b_gradient:.5f}\")\n",
        "            print(f\"Iteration {iteration:5} : cost = {cost:.2f}\")\n",
        "            print('----------------------------------------------------')\n",
        "\n",
        "    return w, b, cost_hist, w_hist, b_hist\n",
        "\n",
        "def MakePrediction(w, b, test_dataset):\n",
        "    \"\"\"\n",
        "    使用迴歸模型進行預測。\n",
        "\n",
        "    參數:\n",
        "    - w (float): 權重。\n",
        "    - b (float): 偏差。\n",
        "    - test_dataset (numpy.ndarray): 測試資料的輸入特徵。\n",
        "\n",
        "    回傳:\n",
        "    - predictions (numpy.ndarray): 預測值。\n",
        "    \"\"\"\n",
        "    test_dataset = np.array(test_dataset).flatten()\n",
        "    predictions = w * test_dataset + b\n",
        "    return predictions\n",
        "\n",
        "def MAPE(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    計算平均絕對百分比誤差（MAPE）。\n",
        "\n",
        "    參數:\n",
        "    - y_true (numpy.ndarray): 真實目標值。\n",
        "    - y_pred (numpy.ndarray): 預測目標值。\n",
        "\n",
        "    回傳:\n",
        "    - mape (float): MAPE值（百分比）。\n",
        "    \"\"\"\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    # 避免除以零\n",
        "    y_true_safe = np.where(y_true == 0, np.mean(y_true[y_true != 0]), y_true)\n",
        "    y_diff = np.abs(y_true_safe - y_pred) / y_true_safe\n",
        "    mape = np.mean(y_diff) * 100\n",
        "    return mape\n",
        "\n",
        "# 資料處理步驟\n",
        "\n",
        "# 1. 分割資料\n",
        "split_ratio = 0.9\n",
        "training_data, validation_data = SplitData(training_datalist, split_ratio)\n",
        "\n",
        "# 2. 預處理資料\n",
        "training_data = PreprocessData(training_data)\n",
        "validation_data = PreprocessData(validation_data)\n",
        "\n",
        "# 3. 標準化特徵\n",
        "scaler = StandardScaler()\n",
        "training_data_x = training_data[:, :-1]\n",
        "training_data_y = training_data[:, -1]\n",
        "scaler.fit(training_data_x)  # 使用訓練集擬合\n",
        "training_data_x = scaler.transform(training_data_x)\n",
        "training_data = np.hstack((training_data_x, training_data_y.reshape(-1, 1)))\n",
        "\n",
        "validation_data_x = validation_data[:, :-1]\n",
        "validation_data_y = validation_data[:, -1]\n",
        "validation_data_x = scaler.transform(validation_data_x)\n",
        "validation_data = np.hstack((validation_data_x, validation_data_y.reshape(-1, 1)))\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "print(\"Training data shape:\", training_data.shape)\n",
        "print(\"Validation data shape:\", validation_data.shape)\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# 分離特徵與標籤\n",
        "X_train = training_data[:, :-1]\n",
        "y_train = training_data[:, -1]\n",
        "X_val = validation_data[:, :-1]\n",
        "y_val = validation_data[:, -1]\n",
        "\n",
        "# 4. 訓練迴歸模型\n",
        "w_final, b_final, c_hist, w_hist, b_hist = Regression(training_data)\n",
        "print(f\"Final weights -> w: {w_final:.5f}, b: {b_final:.5f}\")\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# 5. 預測驗證資料並計算MAPE\n",
        "prediction_data = MakePrediction(w_final, b_final, X_val)\n",
        "mape = MAPE(y_val, prediction_data)\n",
        "print(f\"MAPE: {mape:.5f}%\")\n",
        "\n",
        "# 6. 預測測試資料並儲存結果\n",
        "# 假設測試集只有特徵，無標籤\n",
        "testing_data_x = testing_datalist\n",
        "testing_data_x = scaler.transform(testing_data_x)  # 使用訓練集的 scaler 進行標準化\n",
        "prediction_ans = MakePrediction(w_final, b_final, testing_data_x)\n",
        "output_datalist = prediction_ans.reshape(-1, 1)\n",
        "\n",
        "# 將預測結果寫入CSV檔案\n",
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Id', 'gripForce'])\n",
        "    for i, prediction in enumerate(output_datalist):\n",
        "        writer.writerow([i, prediction[0]])\n",
        "\n",
        "\n",
        "# 最好的測試結果\n",
        "    # num_iteration = 70000\n",
        "    # learning_rate_w = 0.000102  # 權重的學習率\n",
        "    # learning_rate_b = 0.000201   # 偏差的學習率"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIZJ7OKAxypP",
        "outputId": "0ec00062-7c07-44da-c0b7-876e234e4292"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted 70 rows containing missing values.\n",
            "Column 0: Q1 = 57.30, Q3 = 75.70, IQR = 18.40\n",
            "Outlier range for column 0: [29.70, 103.30]\n",
            "Number of outliers in column 0: 560\n",
            "Column 1: Q1 = 33.20, Q3 = 55.40, IQR = 22.20\n",
            "Outlier range for column 1: [-0.10, 88.70]\n",
            "Number of outliers in column 1: 446\n",
            "Deleted 982 rows containing outliers.\n",
            "Preprocessed data shape: (7948, 2)\n",
            "Deleted 3 rows containing missing values.\n",
            "Column 0: Q1 = 58.70, Q3 = 76.10, IQR = 17.40\n",
            "Outlier range for column 0: [32.60, 102.20]\n",
            "Number of outliers in column 0: 63\n",
            "Column 1: Q1 = 33.80, Q3 = 56.00, IQR = 22.20\n",
            "Outlier range for column 1: [0.50, 89.30]\n",
            "Number of outliers in column 1: 50\n",
            "Deleted 108 rows containing outliers.\n",
            "Preprocessed data shape: (889, 2)\n",
            "-----------------------------------------------\n",
            "Training data shape: (7948, 2)\n",
            "Validation data shape: (889, 2)\n",
            "-----------------------------------------------\n",
            "w = -9.99807, b = 0.00886\n",
            "w_gradient = -18.97008, b_gradient = -44.06436\n",
            "Iteration     0 : cost = 2381.66\n",
            "----------------------------------------------------\n",
            "w = -9.61502, b = 1.74493\n",
            "w_gradient = -18.58699, b_gradient = -42.32793\n",
            "Iteration   200 : cost = 2217.32\n",
            "----------------------------------------------------\n",
            "w = -9.23970, b = 3.41260\n",
            "w_gradient = -18.21164, b_gradient = -40.65993\n",
            "Iteration   400 : cost = 2065.14\n",
            "----------------------------------------------------\n",
            "w = -8.87197, b = 5.01454\n",
            "w_gradient = -17.84387, b_gradient = -39.05766\n",
            "Iteration   600 : cost = 1924.21\n",
            "----------------------------------------------------\n",
            "w = -8.51166, b = 6.55336\n",
            "w_gradient = -17.48352, b_gradient = -37.51853\n",
            "Iteration   800 : cost = 1793.67\n",
            "----------------------------------------------------\n",
            "w = -8.15863, b = 8.03154\n",
            "w_gradient = -17.13046, b_gradient = -36.04006\n",
            "Iteration  1000 : cost = 1672.74\n",
            "----------------------------------------------------\n",
            "w = -7.81272, b = 9.45147\n",
            "w_gradient = -16.78452, b_gradient = -34.61984\n",
            "Iteration  1200 : cost = 1560.70\n",
            "----------------------------------------------------\n",
            "w = -7.47381, b = 10.81545\n",
            "w_gradient = -16.44556, b_gradient = -33.25559\n",
            "Iteration  1400 : cost = 1456.87\n",
            "----------------------------------------------------\n",
            "w = -7.14173, b = 12.12567\n",
            "w_gradient = -16.11346, b_gradient = -31.94510\n",
            "Iteration  1600 : cost = 1360.65\n",
            "----------------------------------------------------\n",
            "w = -6.81636, b = 13.38427\n",
            "w_gradient = -15.78806, b_gradient = -30.68625\n",
            "Iteration  1800 : cost = 1271.46\n",
            "----------------------------------------------------\n",
            "w = -6.49757, b = 14.59327\n",
            "w_gradient = -15.46923, b_gradient = -29.47701\n",
            "Iteration  2000 : cost = 1188.78\n",
            "----------------------------------------------------\n",
            "w = -6.18521, b = 15.75462\n",
            "w_gradient = -15.15684, b_gradient = -28.31543\n",
            "Iteration  2200 : cost = 1112.11\n",
            "----------------------------------------------------\n",
            "w = -5.87916, b = 16.87021\n",
            "w_gradient = -14.85075, b_gradient = -27.19961\n",
            "Iteration  2400 : cost = 1041.00\n",
            "----------------------------------------------------\n",
            "w = -5.57929, b = 17.94184\n",
            "w_gradient = -14.55085, b_gradient = -26.12777\n",
            "Iteration  2600 : cost = 975.05\n",
            "----------------------------------------------------\n",
            "w = -5.28547, b = 18.97124\n",
            "w_gradient = -14.25701, b_gradient = -25.09816\n",
            "Iteration  2800 : cost = 913.87\n",
            "----------------------------------------------------\n",
            "w = -4.99759, b = 19.96007\n",
            "w_gradient = -13.96910, b_gradient = -24.10913\n",
            "Iteration  3000 : cost = 857.09\n",
            "----------------------------------------------------\n",
            "w = -4.71552, b = 20.90994\n",
            "w_gradient = -13.68700, b_gradient = -23.15907\n",
            "Iteration  3200 : cost = 804.41\n",
            "----------------------------------------------------\n",
            "w = -4.43915, b = 21.82238\n",
            "w_gradient = -13.41060, b_gradient = -22.24645\n",
            "Iteration  3400 : cost = 755.50\n",
            "----------------------------------------------------\n",
            "w = -4.16836, b = 22.69886\n",
            "w_gradient = -13.13978, b_gradient = -21.36979\n",
            "Iteration  3600 : cost = 710.09\n",
            "----------------------------------------------------\n",
            "w = -3.90304, b = 23.54080\n",
            "w_gradient = -12.87443, b_gradient = -20.52768\n",
            "Iteration  3800 : cost = 667.92\n",
            "----------------------------------------------------\n",
            "w = -3.64307, b = 24.34956\n",
            "w_gradient = -12.61444, b_gradient = -19.71876\n",
            "Iteration  4000 : cost = 628.75\n",
            "----------------------------------------------------\n",
            "w = -3.38836, b = 25.12645\n",
            "w_gradient = -12.35970, b_gradient = -18.94171\n",
            "Iteration  4200 : cost = 592.36\n",
            "----------------------------------------------------\n",
            "w = -3.13879, b = 25.87273\n",
            "w_gradient = -12.11010, b_gradient = -18.19528\n",
            "Iteration  4400 : cost = 558.54\n",
            "----------------------------------------------------\n",
            "w = -2.89426, b = 26.58960\n",
            "w_gradient = -11.86555, b_gradient = -17.47827\n",
            "Iteration  4600 : cost = 527.11\n",
            "----------------------------------------------------\n",
            "w = -2.65466, b = 27.27822\n",
            "w_gradient = -11.62593, b_gradient = -16.78951\n",
            "Iteration  4800 : cost = 497.89\n",
            "----------------------------------------------------\n",
            "w = -2.41991, b = 27.93971\n",
            "w_gradient = -11.39115, b_gradient = -16.12789\n",
            "Iteration  5000 : cost = 470.72\n",
            "----------------------------------------------------\n",
            "w = -2.18990, b = 28.57512\n",
            "w_gradient = -11.16112, b_gradient = -15.49235\n",
            "Iteration  5200 : cost = 445.44\n",
            "----------------------------------------------------\n",
            "w = -1.96453, b = 29.18550\n",
            "w_gradient = -10.93573, b_gradient = -14.88185\n",
            "Iteration  5400 : cost = 421.93\n",
            "----------------------------------------------------\n",
            "w = -1.74371, b = 29.77183\n",
            "w_gradient = -10.71489, b_gradient = -14.29540\n",
            "Iteration  5600 : cost = 400.04\n",
            "----------------------------------------------------\n",
            "w = -1.52735, b = 30.33505\n",
            "w_gradient = -10.49851, b_gradient = -13.73207\n",
            "Iteration  5800 : cost = 379.67\n",
            "----------------------------------------------------\n",
            "w = -1.31536, b = 30.87607\n",
            "w_gradient = -10.28650, b_gradient = -13.19094\n",
            "Iteration  6000 : cost = 360.70\n",
            "----------------------------------------------------\n",
            "w = -1.10766, b = 31.39578\n",
            "w_gradient = -10.07877, b_gradient = -12.67113\n",
            "Iteration  6200 : cost = 343.04\n",
            "----------------------------------------------------\n",
            "w = -0.90414, b = 31.89500\n",
            "w_gradient = -9.87523, b_gradient = -12.17180\n",
            "Iteration  6400 : cost = 326.58\n",
            "----------------------------------------------------\n",
            "w = -0.70474, b = 32.37455\n",
            "w_gradient = -9.67581, b_gradient = -11.69215\n",
            "Iteration  6600 : cost = 311.24\n",
            "----------------------------------------------------\n",
            "w = -0.50936, b = 32.83521\n",
            "w_gradient = -9.48041, b_gradient = -11.23140\n",
            "Iteration  6800 : cost = 296.94\n",
            "----------------------------------------------------\n",
            "w = -0.31793, b = 33.27771\n",
            "w_gradient = -9.28896, b_gradient = -10.78881\n",
            "Iteration  7000 : cost = 283.60\n",
            "----------------------------------------------------\n",
            "w = -0.13037, b = 33.70278\n",
            "w_gradient = -9.10138, b_gradient = -10.36366\n",
            "Iteration  7200 : cost = 271.16\n",
            "----------------------------------------------------\n",
            "w = 0.05341, b = 34.11109\n",
            "w_gradient = -8.91758, b_gradient = -9.95527\n",
            "Iteration  7400 : cost = 259.56\n",
            "----------------------------------------------------\n",
            "w = 0.23348, b = 34.50331\n",
            "w_gradient = -8.73750, b_gradient = -9.56296\n",
            "Iteration  7600 : cost = 248.72\n",
            "----------------------------------------------------\n",
            "w = 0.40991, b = 34.88008\n",
            "w_gradient = -8.56105, b_gradient = -9.18612\n",
            "Iteration  7800 : cost = 238.61\n",
            "----------------------------------------------------\n",
            "w = 0.58278, b = 35.24200\n",
            "w_gradient = -8.38816, b_gradient = -8.82413\n",
            "Iteration  8000 : cost = 229.16\n",
            "----------------------------------------------------\n",
            "w = 0.75215, b = 35.58966\n",
            "w_gradient = -8.21877, b_gradient = -8.47640\n",
            "Iteration  8200 : cost = 220.34\n",
            "----------------------------------------------------\n",
            "w = 0.91811, b = 35.92362\n",
            "w_gradient = -8.05280, b_gradient = -8.14237\n",
            "Iteration  8400 : cost = 212.09\n",
            "----------------------------------------------------\n",
            "w = 1.08071, b = 36.24442\n",
            "w_gradient = -7.89018, b_gradient = -7.82151\n",
            "Iteration  8600 : cost = 204.38\n",
            "----------------------------------------------------\n",
            "w = 1.24003, b = 36.55258\n",
            "w_gradient = -7.73084, b_gradient = -7.51329\n",
            "Iteration  8800 : cost = 197.16\n",
            "----------------------------------------------------\n",
            "w = 1.39614, b = 36.84859\n",
            "w_gradient = -7.57472, b_gradient = -7.21722\n",
            "Iteration  9000 : cost = 190.41\n",
            "----------------------------------------------------\n",
            "w = 1.54909, b = 37.13294\n",
            "w_gradient = -7.42175, b_gradient = -6.93281\n",
            "Iteration  9200 : cost = 184.10\n",
            "----------------------------------------------------\n",
            "w = 1.69895, b = 37.40608\n",
            "w_gradient = -7.27188, b_gradient = -6.65961\n",
            "Iteration  9400 : cost = 178.18\n",
            "----------------------------------------------------\n",
            "w = 1.84578, b = 37.66846\n",
            "w_gradient = -7.12502, b_gradient = -6.39718\n",
            "Iteration  9600 : cost = 172.65\n",
            "----------------------------------------------------\n",
            "w = 1.98965, b = 37.92050\n",
            "w_gradient = -6.98114, b_gradient = -6.14509\n",
            "Iteration  9800 : cost = 167.46\n",
            "----------------------------------------------------\n",
            "w = 2.13062, b = 38.16261\n",
            "w_gradient = -6.84016, b_gradient = -5.90293\n",
            "Iteration 10000 : cost = 162.59\n",
            "----------------------------------------------------\n",
            "w = 2.26874, b = 38.39518\n",
            "w_gradient = -6.70203, b_gradient = -5.67032\n",
            "Iteration 10200 : cost = 158.03\n",
            "----------------------------------------------------\n",
            "w = 2.40407, b = 38.61858\n",
            "w_gradient = -6.56668, b_gradient = -5.44687\n",
            "Iteration 10400 : cost = 153.75\n",
            "----------------------------------------------------\n",
            "w = 2.53666, b = 38.83318\n",
            "w_gradient = -6.43407, b_gradient = -5.23223\n",
            "Iteration 10600 : cost = 149.74\n",
            "----------------------------------------------------\n",
            "w = 2.66658, b = 39.03932\n",
            "w_gradient = -6.30414, b_gradient = -5.02604\n",
            "Iteration 10800 : cost = 145.97\n",
            "----------------------------------------------------\n",
            "w = 2.79388, b = 39.23734\n",
            "w_gradient = -6.17683, b_gradient = -4.82798\n",
            "Iteration 11000 : cost = 142.43\n",
            "----------------------------------------------------\n",
            "w = 2.91860, b = 39.42756\n",
            "w_gradient = -6.05210, b_gradient = -4.63773\n",
            "Iteration 11200 : cost = 139.10\n",
            "----------------------------------------------------\n",
            "w = 3.04081, b = 39.61028\n",
            "w_gradient = -5.92988, b_gradient = -4.45497\n",
            "Iteration 11400 : cost = 135.98\n",
            "----------------------------------------------------\n",
            "w = 3.16055, b = 39.78580\n",
            "w_gradient = -5.81013, b_gradient = -4.27942\n",
            "Iteration 11600 : cost = 133.04\n",
            "----------------------------------------------------\n",
            "w = 3.27787, b = 39.95440\n",
            "w_gradient = -5.69280, b_gradient = -4.11078\n",
            "Iteration 11800 : cost = 130.28\n",
            "----------------------------------------------------\n",
            "w = 3.39282, b = 40.11636\n",
            "w_gradient = -5.57783, b_gradient = -3.94879\n",
            "Iteration 12000 : cost = 127.68\n",
            "----------------------------------------------------\n",
            "w = 3.50545, b = 40.27194\n",
            "w_gradient = -5.46519, b_gradient = -3.79318\n",
            "Iteration 12200 : cost = 125.23\n",
            "----------------------------------------------------\n",
            "w = 3.61580, b = 40.42138\n",
            "w_gradient = -5.35483, b_gradient = -3.64370\n",
            "Iteration 12400 : cost = 122.92\n",
            "----------------------------------------------------\n",
            "w = 3.72393, b = 40.56494\n",
            "w_gradient = -5.24669, b_gradient = -3.50012\n",
            "Iteration 12600 : cost = 120.75\n",
            "----------------------------------------------------\n",
            "w = 3.82987, b = 40.70284\n",
            "w_gradient = -5.14074, b_gradient = -3.36219\n",
            "Iteration 12800 : cost = 118.70\n",
            "----------------------------------------------------\n",
            "w = 3.93367, b = 40.83531\n",
            "w_gradient = -5.03692, b_gradient = -3.22970\n",
            "Iteration 13000 : cost = 116.77\n",
            "----------------------------------------------------\n",
            "w = 4.03538, b = 40.96255\n",
            "w_gradient = -4.93521, b_gradient = -3.10243\n",
            "Iteration 13200 : cost = 114.95\n",
            "----------------------------------------------------\n",
            "w = 4.13503, b = 41.08478\n",
            "w_gradient = -4.83554, b_gradient = -2.98017\n",
            "Iteration 13400 : cost = 113.24\n",
            "----------------------------------------------------\n",
            "w = 4.23267, b = 41.20220\n",
            "w_gradient = -4.73789, b_gradient = -2.86273\n",
            "Iteration 13600 : cost = 111.62\n",
            "----------------------------------------------------\n",
            "w = 4.32834, b = 41.31499\n",
            "w_gradient = -4.64221, b_gradient = -2.74992\n",
            "Iteration 13800 : cost = 110.09\n",
            "----------------------------------------------------\n",
            "w = 4.42208, b = 41.42333\n",
            "w_gradient = -4.54847, b_gradient = -2.64156\n",
            "Iteration 14000 : cost = 108.64\n",
            "----------------------------------------------------\n",
            "w = 4.51392, b = 41.52740\n",
            "w_gradient = -4.45661, b_gradient = -2.53746\n",
            "Iteration 14200 : cost = 107.28\n",
            "----------------------------------------------------\n",
            "w = 4.60391, b = 41.62738\n",
            "w_gradient = -4.36662, b_gradient = -2.43747\n",
            "Iteration 14400 : cost = 105.98\n",
            "----------------------------------------------------\n",
            "w = 4.69208, b = 41.72341\n",
            "w_gradient = -4.27843, b_gradient = -2.34142\n",
            "Iteration 14600 : cost = 104.76\n",
            "----------------------------------------------------\n",
            "w = 4.77848, b = 41.81566\n",
            "w_gradient = -4.19203, b_gradient = -2.24915\n",
            "Iteration 14800 : cost = 103.61\n",
            "----------------------------------------------------\n",
            "w = 4.86312, b = 41.90427\n",
            "w_gradient = -4.10738, b_gradient = -2.16052\n",
            "Iteration 15000 : cost = 102.52\n",
            "----------------------------------------------------\n",
            "w = 4.94606, b = 41.98939\n",
            "w_gradient = -4.02443, b_gradient = -2.07538\n",
            "Iteration 15200 : cost = 101.48\n",
            "----------------------------------------------------\n",
            "w = 5.02732, b = 42.07116\n",
            "w_gradient = -3.94316, b_gradient = -1.99360\n",
            "Iteration 15400 : cost = 100.50\n",
            "----------------------------------------------------\n",
            "w = 5.10694, b = 42.14971\n",
            "w_gradient = -3.86353, b_gradient = -1.91504\n",
            "Iteration 15600 : cost = 99.57\n",
            "----------------------------------------------------\n",
            "w = 5.18496, b = 42.22516\n",
            "w_gradient = -3.78551, b_gradient = -1.83957\n",
            "Iteration 15800 : cost = 98.69\n",
            "----------------------------------------------------\n",
            "w = 5.26140, b = 42.29763\n",
            "w_gradient = -3.70907, b_gradient = -1.76708\n",
            "Iteration 16000 : cost = 97.86\n",
            "----------------------------------------------------\n",
            "w = 5.33629, b = 42.36725\n",
            "w_gradient = -3.63416, b_gradient = -1.69744\n",
            "Iteration 16200 : cost = 97.07\n",
            "----------------------------------------------------\n",
            "w = 5.40967, b = 42.43413\n",
            "w_gradient = -3.56077, b_gradient = -1.63055\n",
            "Iteration 16400 : cost = 96.32\n",
            "----------------------------------------------------\n",
            "w = 5.48157, b = 42.49837\n",
            "w_gradient = -3.48887, b_gradient = -1.56630\n",
            "Iteration 16600 : cost = 95.60\n",
            "----------------------------------------------------\n",
            "w = 5.55202, b = 42.56008\n",
            "w_gradient = -3.41841, b_gradient = -1.50458\n",
            "Iteration 16800 : cost = 94.93\n",
            "----------------------------------------------------\n",
            "w = 5.62105, b = 42.61936\n",
            "w_gradient = -3.34938, b_gradient = -1.44529\n",
            "Iteration 17000 : cost = 94.29\n",
            "----------------------------------------------------\n",
            "w = 5.68868, b = 42.67630\n",
            "w_gradient = -3.28174, b_gradient = -1.38833\n",
            "Iteration 17200 : cost = 93.68\n",
            "----------------------------------------------------\n",
            "w = 5.75494, b = 42.73100\n",
            "w_gradient = -3.21547, b_gradient = -1.33362\n",
            "Iteration 17400 : cost = 93.10\n",
            "----------------------------------------------------\n",
            "w = 5.81987, b = 42.78354\n",
            "w_gradient = -3.15053, b_gradient = -1.28107\n",
            "Iteration 17600 : cost = 92.55\n",
            "----------------------------------------------------\n",
            "w = 5.88349, b = 42.83402\n",
            "w_gradient = -3.08691, b_gradient = -1.23059\n",
            "Iteration 17800 : cost = 92.02\n",
            "----------------------------------------------------\n",
            "w = 5.94582, b = 42.88250\n",
            "w_gradient = -3.02457, b_gradient = -1.18209\n",
            "Iteration 18000 : cost = 91.53\n",
            "----------------------------------------------------\n",
            "w = 6.00689, b = 42.92907\n",
            "w_gradient = -2.96349, b_gradient = -1.13551\n",
            "Iteration 18200 : cost = 91.05\n",
            "----------------------------------------------------\n",
            "w = 6.06673, b = 42.97381\n",
            "w_gradient = -2.90365, b_gradient = -1.09077\n",
            "Iteration 18400 : cost = 90.60\n",
            "----------------------------------------------------\n",
            "w = 6.12536, b = 43.01678\n",
            "w_gradient = -2.84501, b_gradient = -1.04778\n",
            "Iteration 18600 : cost = 90.17\n",
            "----------------------------------------------------\n",
            "w = 6.18281, b = 43.05807\n",
            "w_gradient = -2.78756, b_gradient = -1.00649\n",
            "Iteration 18800 : cost = 89.76\n",
            "----------------------------------------------------\n",
            "w = 6.23910, b = 43.09772\n",
            "w_gradient = -2.73126, b_gradient = -0.96683\n",
            "Iteration 19000 : cost = 89.38\n",
            "----------------------------------------------------\n",
            "w = 6.29425, b = 43.13581\n",
            "w_gradient = -2.67611, b_gradient = -0.92873\n",
            "Iteration 19200 : cost = 89.00\n",
            "----------------------------------------------------\n",
            "w = 6.34828, b = 43.17240\n",
            "w_gradient = -2.62207, b_gradient = -0.89213\n",
            "Iteration 19400 : cost = 88.65\n",
            "----------------------------------------------------\n",
            "w = 6.40123, b = 43.20755\n",
            "w_gradient = -2.56911, b_gradient = -0.85698\n",
            "Iteration 19600 : cost = 88.32\n",
            "----------------------------------------------------\n",
            "w = 6.45311, b = 43.24132\n",
            "w_gradient = -2.51723, b_gradient = -0.82321\n",
            "Iteration 19800 : cost = 88.00\n",
            "----------------------------------------------------\n",
            "w = 6.50393, b = 43.27375\n",
            "w_gradient = -2.46640, b_gradient = -0.79077\n",
            "Iteration 20000 : cost = 87.69\n",
            "----------------------------------------------------\n",
            "w = 6.55374, b = 43.30490\n",
            "w_gradient = -2.41659, b_gradient = -0.75960\n",
            "Iteration 20200 : cost = 87.40\n",
            "----------------------------------------------------\n",
            "w = 6.60253, b = 43.33483\n",
            "w_gradient = -2.36779, b_gradient = -0.72967\n",
            "Iteration 20400 : cost = 87.12\n",
            "----------------------------------------------------\n",
            "w = 6.65034, b = 43.36358\n",
            "w_gradient = -2.31997, b_gradient = -0.70092\n",
            "Iteration 20600 : cost = 86.85\n",
            "----------------------------------------------------\n",
            "w = 6.69719, b = 43.39119\n",
            "w_gradient = -2.27312, b_gradient = -0.67330\n",
            "Iteration 20800 : cost = 86.60\n",
            "----------------------------------------------------\n",
            "w = 6.74309, b = 43.41772\n",
            "w_gradient = -2.22722, b_gradient = -0.64676\n",
            "Iteration 21000 : cost = 86.36\n",
            "----------------------------------------------------\n",
            "w = 6.78806, b = 43.44320\n",
            "w_gradient = -2.18224, b_gradient = -0.62128\n",
            "Iteration 21200 : cost = 86.13\n",
            "----------------------------------------------------\n",
            "w = 6.83213, b = 43.46768\n",
            "w_gradient = -2.13817, b_gradient = -0.59680\n",
            "Iteration 21400 : cost = 85.91\n",
            "----------------------------------------------------\n",
            "w = 6.87530, b = 43.49119\n",
            "w_gradient = -2.09499, b_gradient = -0.57328\n",
            "Iteration 21600 : cost = 85.70\n",
            "----------------------------------------------------\n",
            "w = 6.91760, b = 43.51378\n",
            "w_gradient = -2.05269, b_gradient = -0.55069\n",
            "Iteration 21800 : cost = 85.50\n",
            "----------------------------------------------------\n",
            "w = 6.95905, b = 43.53548\n",
            "w_gradient = -2.01123, b_gradient = -0.52899\n",
            "Iteration 22000 : cost = 85.31\n",
            "----------------------------------------------------\n",
            "w = 6.99966, b = 43.55632\n",
            "w_gradient = -1.97062, b_gradient = -0.50814\n",
            "Iteration 22200 : cost = 85.12\n",
            "----------------------------------------------------\n",
            "w = 7.03946, b = 43.57634\n",
            "w_gradient = -1.93082, b_gradient = -0.48812\n",
            "Iteration 22400 : cost = 84.95\n",
            "----------------------------------------------------\n",
            "w = 7.07844, b = 43.59557\n",
            "w_gradient = -1.89183, b_gradient = -0.46888\n",
            "Iteration 22600 : cost = 84.78\n",
            "----------------------------------------------------\n",
            "w = 7.11664, b = 43.61404\n",
            "w_gradient = -1.85363, b_gradient = -0.45040\n",
            "Iteration 22800 : cost = 84.62\n",
            "----------------------------------------------------\n",
            "w = 7.15407, b = 43.63179\n",
            "w_gradient = -1.81619, b_gradient = -0.43266\n",
            "Iteration 23000 : cost = 84.47\n",
            "----------------------------------------------------\n",
            "w = 7.19075, b = 43.64883\n",
            "w_gradient = -1.77952, b_gradient = -0.41561\n",
            "Iteration 23200 : cost = 84.32\n",
            "----------------------------------------------------\n",
            "w = 7.22668, b = 43.66521\n",
            "w_gradient = -1.74358, b_gradient = -0.39923\n",
            "Iteration 23400 : cost = 84.18\n",
            "----------------------------------------------------\n",
            "w = 7.26189, b = 43.68094\n",
            "w_gradient = -1.70837, b_gradient = -0.38350\n",
            "Iteration 23600 : cost = 84.05\n",
            "----------------------------------------------------\n",
            "w = 7.29638, b = 43.69605\n",
            "w_gradient = -1.67387, b_gradient = -0.36838\n",
            "Iteration 23800 : cost = 83.92\n",
            "----------------------------------------------------\n",
            "w = 7.33018, b = 43.71056\n",
            "w_gradient = -1.64007, b_gradient = -0.35387\n",
            "Iteration 24000 : cost = 83.80\n",
            "----------------------------------------------------\n",
            "w = 7.36330, b = 43.72450\n",
            "w_gradient = -1.60695, b_gradient = -0.33992\n",
            "Iteration 24200 : cost = 83.68\n",
            "----------------------------------------------------\n",
            "w = 7.39575, b = 43.73789\n",
            "w_gradient = -1.57450, b_gradient = -0.32653\n",
            "Iteration 24400 : cost = 83.57\n",
            "----------------------------------------------------\n",
            "w = 7.42754, b = 43.75076\n",
            "w_gradient = -1.54270, b_gradient = -0.31366\n",
            "Iteration 24600 : cost = 83.46\n",
            "----------------------------------------------------\n",
            "w = 7.45869, b = 43.76312\n",
            "w_gradient = -1.51155, b_gradient = -0.30130\n",
            "Iteration 24800 : cost = 83.36\n",
            "----------------------------------------------------\n",
            "w = 7.48921, b = 43.77499\n",
            "w_gradient = -1.48102, b_gradient = -0.28943\n",
            "Iteration 25000 : cost = 83.26\n",
            "----------------------------------------------------\n",
            "w = 7.51912, b = 43.78639\n",
            "w_gradient = -1.45111, b_gradient = -0.27802\n",
            "Iteration 25200 : cost = 83.17\n",
            "----------------------------------------------------\n",
            "w = 7.54842, b = 43.79734\n",
            "w_gradient = -1.42181, b_gradient = -0.26707\n",
            "Iteration 25400 : cost = 83.07\n",
            "----------------------------------------------------\n",
            "w = 7.57713, b = 43.80787\n",
            "w_gradient = -1.39310, b_gradient = -0.25654\n",
            "Iteration 25600 : cost = 82.99\n",
            "----------------------------------------------------\n",
            "w = 7.60526, b = 43.81797\n",
            "w_gradient = -1.36497, b_gradient = -0.24643\n",
            "Iteration 25800 : cost = 82.91\n",
            "----------------------------------------------------\n",
            "w = 7.63282, b = 43.82768\n",
            "w_gradient = -1.33740, b_gradient = -0.23672\n",
            "Iteration 26000 : cost = 82.83\n",
            "----------------------------------------------------\n",
            "w = 7.65982, b = 43.83701\n",
            "w_gradient = -1.31039, b_gradient = -0.22739\n",
            "Iteration 26200 : cost = 82.75\n",
            "----------------------------------------------------\n",
            "w = 7.68628, b = 43.84597\n",
            "w_gradient = -1.28393, b_gradient = -0.21843\n",
            "Iteration 26400 : cost = 82.68\n",
            "----------------------------------------------------\n",
            "w = 7.71221, b = 43.85457\n",
            "w_gradient = -1.25800, b_gradient = -0.20982\n",
            "Iteration 26600 : cost = 82.61\n",
            "----------------------------------------------------\n",
            "w = 7.73761, b = 43.86284\n",
            "w_gradient = -1.23260, b_gradient = -0.20156\n",
            "Iteration 26800 : cost = 82.54\n",
            "----------------------------------------------------\n",
            "w = 7.76250, b = 43.87078\n",
            "w_gradient = -1.20771, b_gradient = -0.19361\n",
            "Iteration 27000 : cost = 82.48\n",
            "----------------------------------------------------\n",
            "w = 7.78689, b = 43.87841\n",
            "w_gradient = -1.18332, b_gradient = -0.18598\n",
            "Iteration 27200 : cost = 82.42\n",
            "----------------------------------------------------\n",
            "w = 7.81078, b = 43.88574\n",
            "w_gradient = -1.15942, b_gradient = -0.17865\n",
            "Iteration 27400 : cost = 82.36\n",
            "----------------------------------------------------\n",
            "w = 7.83419, b = 43.89278\n",
            "w_gradient = -1.13601, b_gradient = -0.17161\n",
            "Iteration 27600 : cost = 82.30\n",
            "----------------------------------------------------\n",
            "w = 7.85713, b = 43.89954\n",
            "w_gradient = -1.11307, b_gradient = -0.16485\n",
            "Iteration 27800 : cost = 82.25\n",
            "----------------------------------------------------\n",
            "w = 7.87960, b = 43.90603\n",
            "w_gradient = -1.09059, b_gradient = -0.15836\n",
            "Iteration 28000 : cost = 82.20\n",
            "----------------------------------------------------\n",
            "w = 7.90163, b = 43.91227\n",
            "w_gradient = -1.06856, b_gradient = -0.15211\n",
            "Iteration 28200 : cost = 82.15\n",
            "----------------------------------------------------\n",
            "w = 7.92320, b = 43.91826\n",
            "w_gradient = -1.04699, b_gradient = -0.14612\n",
            "Iteration 28400 : cost = 82.10\n",
            "----------------------------------------------------\n",
            "w = 7.94434, b = 43.92402\n",
            "w_gradient = -1.02584, b_gradient = -0.14036\n",
            "Iteration 28600 : cost = 82.05\n",
            "----------------------------------------------------\n",
            "w = 7.96506, b = 43.92955\n",
            "w_gradient = -1.00513, b_gradient = -0.13483\n",
            "Iteration 28800 : cost = 82.01\n",
            "----------------------------------------------------\n",
            "w = 7.98535, b = 43.93486\n",
            "w_gradient = -0.98483, b_gradient = -0.12952\n",
            "Iteration 29000 : cost = 81.97\n",
            "----------------------------------------------------\n",
            "w = 8.00524, b = 43.93997\n",
            "w_gradient = -0.96494, b_gradient = -0.12441\n",
            "Iteration 29200 : cost = 81.93\n",
            "----------------------------------------------------\n",
            "w = 8.02472, b = 43.94487\n",
            "w_gradient = -0.94545, b_gradient = -0.11951\n",
            "Iteration 29400 : cost = 81.89\n",
            "----------------------------------------------------\n",
            "w = 8.04382, b = 43.94958\n",
            "w_gradient = -0.92636, b_gradient = -0.11480\n",
            "Iteration 29600 : cost = 81.85\n",
            "----------------------------------------------------\n",
            "w = 8.06252, b = 43.95410\n",
            "w_gradient = -0.90765, b_gradient = -0.11028\n",
            "Iteration 29800 : cost = 81.82\n",
            "----------------------------------------------------\n",
            "w = 8.08085, b = 43.95844\n",
            "w_gradient = -0.88932, b_gradient = -0.10593\n",
            "Iteration 30000 : cost = 81.78\n",
            "----------------------------------------------------\n",
            "w = 8.09881, b = 43.96262\n",
            "w_gradient = -0.87137, b_gradient = -0.10176\n",
            "Iteration 30200 : cost = 81.75\n",
            "----------------------------------------------------\n",
            "w = 8.11640, b = 43.96663\n",
            "w_gradient = -0.85377, b_gradient = -0.09775\n",
            "Iteration 30400 : cost = 81.72\n",
            "----------------------------------------------------\n",
            "w = 8.13364, b = 43.97048\n",
            "w_gradient = -0.83653, b_gradient = -0.09390\n",
            "Iteration 30600 : cost = 81.69\n",
            "----------------------------------------------------\n",
            "w = 8.15053, b = 43.97418\n",
            "w_gradient = -0.81963, b_gradient = -0.09020\n",
            "Iteration 30800 : cost = 81.66\n",
            "----------------------------------------------------\n",
            "w = 8.16708, b = 43.97773\n",
            "w_gradient = -0.80308, b_gradient = -0.08664\n",
            "Iteration 31000 : cost = 81.63\n",
            "----------------------------------------------------\n",
            "w = 8.18330, b = 43.98115\n",
            "w_gradient = -0.78686, b_gradient = -0.08323\n",
            "Iteration 31200 : cost = 81.61\n",
            "----------------------------------------------------\n",
            "w = 8.19919, b = 43.98442\n",
            "w_gradient = -0.77097, b_gradient = -0.07995\n",
            "Iteration 31400 : cost = 81.58\n",
            "----------------------------------------------------\n",
            "w = 8.21475, b = 43.98757\n",
            "w_gradient = -0.75540, b_gradient = -0.07680\n",
            "Iteration 31600 : cost = 81.56\n",
            "----------------------------------------------------\n",
            "w = 8.23001, b = 43.99060\n",
            "w_gradient = -0.74015, b_gradient = -0.07377\n",
            "Iteration 31800 : cost = 81.54\n",
            "----------------------------------------------------\n",
            "w = 8.24495, b = 43.99351\n",
            "w_gradient = -0.72520, b_gradient = -0.07086\n",
            "Iteration 32000 : cost = 81.51\n",
            "----------------------------------------------------\n",
            "w = 8.25960, b = 43.99630\n",
            "w_gradient = -0.71056, b_gradient = -0.06807\n",
            "Iteration 32200 : cost = 81.49\n",
            "----------------------------------------------------\n",
            "w = 8.27394, b = 43.99898\n",
            "w_gradient = -0.69621, b_gradient = -0.06539\n",
            "Iteration 32400 : cost = 81.47\n",
            "----------------------------------------------------\n",
            "w = 8.28800, b = 44.00156\n",
            "w_gradient = -0.68215, b_gradient = -0.06281\n",
            "Iteration 32600 : cost = 81.45\n",
            "----------------------------------------------------\n",
            "w = 8.30178, b = 44.00403\n",
            "w_gradient = -0.66837, b_gradient = -0.06034\n",
            "Iteration 32800 : cost = 81.43\n",
            "----------------------------------------------------\n",
            "w = 8.31527, b = 44.00641\n",
            "w_gradient = -0.65488, b_gradient = -0.05796\n",
            "Iteration 33000 : cost = 81.41\n",
            "----------------------------------------------------\n",
            "w = 8.32850, b = 44.00869\n",
            "w_gradient = -0.64165, b_gradient = -0.05568\n",
            "Iteration 33200 : cost = 81.40\n",
            "----------------------------------------------------\n",
            "w = 8.34145, b = 44.01089\n",
            "w_gradient = -0.62869, b_gradient = -0.05348\n",
            "Iteration 33400 : cost = 81.38\n",
            "----------------------------------------------------\n",
            "w = 8.35415, b = 44.01299\n",
            "w_gradient = -0.61600, b_gradient = -0.05137\n",
            "Iteration 33600 : cost = 81.36\n",
            "----------------------------------------------------\n",
            "w = 8.36659, b = 44.01502\n",
            "w_gradient = -0.60356, b_gradient = -0.04935\n",
            "Iteration 33800 : cost = 81.35\n",
            "----------------------------------------------------\n",
            "w = 8.37877, b = 44.01696\n",
            "w_gradient = -0.59137, b_gradient = -0.04740\n",
            "Iteration 34000 : cost = 81.33\n",
            "----------------------------------------------------\n",
            "w = 8.39071, b = 44.01883\n",
            "w_gradient = -0.57943, b_gradient = -0.04554\n",
            "Iteration 34200 : cost = 81.32\n",
            "----------------------------------------------------\n",
            "w = 8.40241, b = 44.02062\n",
            "w_gradient = -0.56773, b_gradient = -0.04374\n",
            "Iteration 34400 : cost = 81.31\n",
            "----------------------------------------------------\n",
            "w = 8.41388, b = 44.02235\n",
            "w_gradient = -0.55626, b_gradient = -0.04202\n",
            "Iteration 34600 : cost = 81.29\n",
            "----------------------------------------------------\n",
            "w = 8.42511, b = 44.02400\n",
            "w_gradient = -0.54503, b_gradient = -0.04036\n",
            "Iteration 34800 : cost = 81.28\n",
            "----------------------------------------------------\n",
            "w = 8.43611, b = 44.02559\n",
            "w_gradient = -0.53402, b_gradient = -0.03877\n",
            "Iteration 35000 : cost = 81.27\n",
            "----------------------------------------------------\n",
            "w = 8.44690, b = 44.02712\n",
            "w_gradient = -0.52324, b_gradient = -0.03724\n",
            "Iteration 35200 : cost = 81.26\n",
            "----------------------------------------------------\n",
            "w = 8.45746, b = 44.02859\n",
            "w_gradient = -0.51267, b_gradient = -0.03578\n",
            "Iteration 35400 : cost = 81.25\n",
            "----------------------------------------------------\n",
            "w = 8.46782, b = 44.03000\n",
            "w_gradient = -0.50232, b_gradient = -0.03437\n",
            "Iteration 35600 : cost = 81.24\n",
            "----------------------------------------------------\n",
            "w = 8.47796, b = 44.03135\n",
            "w_gradient = -0.49217, b_gradient = -0.03301\n",
            "Iteration 35800 : cost = 81.23\n",
            "----------------------------------------------------\n",
            "w = 8.48790, b = 44.03265\n",
            "w_gradient = -0.48223, b_gradient = -0.03171\n",
            "Iteration 36000 : cost = 81.22\n",
            "----------------------------------------------------\n",
            "w = 8.49763, b = 44.03390\n",
            "w_gradient = -0.47250, b_gradient = -0.03046\n",
            "Iteration 36200 : cost = 81.21\n",
            "----------------------------------------------------\n",
            "w = 8.50717, b = 44.03510\n",
            "w_gradient = -0.46295, b_gradient = -0.02926\n",
            "Iteration 36400 : cost = 81.20\n",
            "----------------------------------------------------\n",
            "w = 8.51652, b = 44.03625\n",
            "w_gradient = -0.45361, b_gradient = -0.02811\n",
            "Iteration 36600 : cost = 81.19\n",
            "----------------------------------------------------\n",
            "w = 8.52568, b = 44.03736\n",
            "w_gradient = -0.44445, b_gradient = -0.02700\n",
            "Iteration 36800 : cost = 81.18\n",
            "----------------------------------------------------\n",
            "w = 8.53466, b = 44.03842\n",
            "w_gradient = -0.43547, b_gradient = -0.02594\n",
            "Iteration 37000 : cost = 81.17\n",
            "----------------------------------------------------\n",
            "w = 8.54345, b = 44.03945\n",
            "w_gradient = -0.42668, b_gradient = -0.02491\n",
            "Iteration 37200 : cost = 81.17\n",
            "----------------------------------------------------\n",
            "w = 8.55207, b = 44.04043\n",
            "w_gradient = -0.41806, b_gradient = -0.02393\n",
            "Iteration 37400 : cost = 81.16\n",
            "----------------------------------------------------\n",
            "w = 8.56051, b = 44.04137\n",
            "w_gradient = -0.40962, b_gradient = -0.02299\n",
            "Iteration 37600 : cost = 81.15\n",
            "----------------------------------------------------\n",
            "w = 8.56878, b = 44.04228\n",
            "w_gradient = -0.40135, b_gradient = -0.02208\n",
            "Iteration 37800 : cost = 81.14\n",
            "----------------------------------------------------\n",
            "w = 8.57688, b = 44.04315\n",
            "w_gradient = -0.39324, b_gradient = -0.02121\n",
            "Iteration 38000 : cost = 81.14\n",
            "----------------------------------------------------\n",
            "w = 8.58482, b = 44.04398\n",
            "w_gradient = -0.38530, b_gradient = -0.02038\n",
            "Iteration 38200 : cost = 81.13\n",
            "----------------------------------------------------\n",
            "w = 8.59260, b = 44.04479\n",
            "w_gradient = -0.37752, b_gradient = -0.01957\n",
            "Iteration 38400 : cost = 81.13\n",
            "----------------------------------------------------\n",
            "w = 8.60023, b = 44.04556\n",
            "w_gradient = -0.36989, b_gradient = -0.01880\n",
            "Iteration 38600 : cost = 81.12\n",
            "----------------------------------------------------\n",
            "w = 8.60769, b = 44.04630\n",
            "w_gradient = -0.36242, b_gradient = -0.01806\n",
            "Iteration 38800 : cost = 81.11\n",
            "----------------------------------------------------\n",
            "w = 8.61501, b = 44.04701\n",
            "w_gradient = -0.35511, b_gradient = -0.01735\n",
            "Iteration 39000 : cost = 81.11\n",
            "----------------------------------------------------\n",
            "w = 8.62218, b = 44.04769\n",
            "w_gradient = -0.34793, b_gradient = -0.01667\n",
            "Iteration 39200 : cost = 81.10\n",
            "----------------------------------------------------\n",
            "w = 8.62921, b = 44.04835\n",
            "w_gradient = -0.34091, b_gradient = -0.01601\n",
            "Iteration 39400 : cost = 81.10\n",
            "----------------------------------------------------\n",
            "w = 8.63609, b = 44.04898\n",
            "w_gradient = -0.33402, b_gradient = -0.01538\n",
            "Iteration 39600 : cost = 81.09\n",
            "----------------------------------------------------\n",
            "w = 8.64284, b = 44.04959\n",
            "w_gradient = -0.32728, b_gradient = -0.01477\n",
            "Iteration 39800 : cost = 81.09\n",
            "----------------------------------------------------\n",
            "w = 8.64945, b = 44.05017\n",
            "w_gradient = -0.32067, b_gradient = -0.01419\n",
            "Iteration 40000 : cost = 81.09\n",
            "----------------------------------------------------\n",
            "w = 8.65592, b = 44.05073\n",
            "w_gradient = -0.31419, b_gradient = -0.01363\n",
            "Iteration 40200 : cost = 81.08\n",
            "----------------------------------------------------\n",
            "w = 8.66226, b = 44.05126\n",
            "w_gradient = -0.30785, b_gradient = -0.01309\n",
            "Iteration 40400 : cost = 81.08\n",
            "----------------------------------------------------\n",
            "w = 8.66848, b = 44.05178\n",
            "w_gradient = -0.30163, b_gradient = -0.01258\n",
            "Iteration 40600 : cost = 81.07\n",
            "----------------------------------------------------\n",
            "w = 8.67457, b = 44.05228\n",
            "w_gradient = -0.29554, b_gradient = -0.01208\n",
            "Iteration 40800 : cost = 81.07\n",
            "----------------------------------------------------\n",
            "w = 8.68054, b = 44.05275\n",
            "w_gradient = -0.28957, b_gradient = -0.01161\n",
            "Iteration 41000 : cost = 81.07\n",
            "----------------------------------------------------\n",
            "w = 8.68639, b = 44.05321\n",
            "w_gradient = -0.28372, b_gradient = -0.01115\n",
            "Iteration 41200 : cost = 81.06\n",
            "----------------------------------------------------\n",
            "w = 8.69212, b = 44.05365\n",
            "w_gradient = -0.27799, b_gradient = -0.01071\n",
            "Iteration 41400 : cost = 81.06\n",
            "----------------------------------------------------\n",
            "w = 8.69773, b = 44.05407\n",
            "w_gradient = -0.27238, b_gradient = -0.01029\n",
            "Iteration 41600 : cost = 81.06\n",
            "----------------------------------------------------\n",
            "w = 8.70323, b = 44.05448\n",
            "w_gradient = -0.26688, b_gradient = -0.00988\n",
            "Iteration 41800 : cost = 81.05\n",
            "----------------------------------------------------\n",
            "w = 8.70862, b = 44.05486\n",
            "w_gradient = -0.26149, b_gradient = -0.00949\n",
            "Iteration 42000 : cost = 81.05\n",
            "----------------------------------------------------\n",
            "w = 8.71390, b = 44.05524\n",
            "w_gradient = -0.25621, b_gradient = -0.00912\n",
            "Iteration 42200 : cost = 81.05\n",
            "----------------------------------------------------\n",
            "w = 8.71907, b = 44.05560\n",
            "w_gradient = -0.25104, b_gradient = -0.00876\n",
            "Iteration 42400 : cost = 81.05\n",
            "----------------------------------------------------\n",
            "w = 8.72414, b = 44.05594\n",
            "w_gradient = -0.24597, b_gradient = -0.00841\n",
            "Iteration 42600 : cost = 81.04\n",
            "----------------------------------------------------\n",
            "w = 8.72911, b = 44.05627\n",
            "w_gradient = -0.24100, b_gradient = -0.00808\n",
            "Iteration 42800 : cost = 81.04\n",
            "----------------------------------------------------\n",
            "w = 8.73397, b = 44.05659\n",
            "w_gradient = -0.23613, b_gradient = -0.00776\n",
            "Iteration 43000 : cost = 81.04\n",
            "----------------------------------------------------\n",
            "w = 8.73874, b = 44.05690\n",
            "w_gradient = -0.23136, b_gradient = -0.00746\n",
            "Iteration 43200 : cost = 81.04\n",
            "----------------------------------------------------\n",
            "w = 8.74341, b = 44.05719\n",
            "w_gradient = -0.22669, b_gradient = -0.00716\n",
            "Iteration 43400 : cost = 81.03\n",
            "----------------------------------------------------\n",
            "w = 8.74799, b = 44.05748\n",
            "w_gradient = -0.22211, b_gradient = -0.00688\n",
            "Iteration 43600 : cost = 81.03\n",
            "----------------------------------------------------\n",
            "w = 8.75248, b = 44.05775\n",
            "w_gradient = -0.21763, b_gradient = -0.00661\n",
            "Iteration 43800 : cost = 81.03\n",
            "----------------------------------------------------\n",
            "w = 8.75687, b = 44.05801\n",
            "w_gradient = -0.21323, b_gradient = -0.00635\n",
            "Iteration 44000 : cost = 81.03\n",
            "----------------------------------------------------\n",
            "w = 8.76118, b = 44.05826\n",
            "w_gradient = -0.20893, b_gradient = -0.00610\n",
            "Iteration 44200 : cost = 81.03\n",
            "----------------------------------------------------\n",
            "w = 8.76539, b = 44.05850\n",
            "w_gradient = -0.20471, b_gradient = -0.00586\n",
            "Iteration 44400 : cost = 81.02\n",
            "----------------------------------------------------\n",
            "w = 8.76953, b = 44.05873\n",
            "w_gradient = -0.20057, b_gradient = -0.00563\n",
            "Iteration 44600 : cost = 81.02\n",
            "----------------------------------------------------\n",
            "w = 8.77358, b = 44.05895\n",
            "w_gradient = -0.19652, b_gradient = -0.00541\n",
            "Iteration 44800 : cost = 81.02\n",
            "----------------------------------------------------\n",
            "w = 8.77755, b = 44.05916\n",
            "w_gradient = -0.19256, b_gradient = -0.00519\n",
            "Iteration 45000 : cost = 81.02\n",
            "----------------------------------------------------\n",
            "w = 8.78143, b = 44.05937\n",
            "w_gradient = -0.18867, b_gradient = -0.00499\n",
            "Iteration 45200 : cost = 81.02\n",
            "----------------------------------------------------\n",
            "w = 8.78524, b = 44.05956\n",
            "w_gradient = -0.18486, b_gradient = -0.00479\n",
            "Iteration 45400 : cost = 81.02\n",
            "----------------------------------------------------\n",
            "w = 8.78898, b = 44.05975\n",
            "w_gradient = -0.18112, b_gradient = -0.00460\n",
            "Iteration 45600 : cost = 81.02\n",
            "----------------------------------------------------\n",
            "w = 8.79263, b = 44.05993\n",
            "w_gradient = -0.17747, b_gradient = -0.00442\n",
            "Iteration 45800 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.79622, b = 44.06011\n",
            "w_gradient = -0.17388, b_gradient = -0.00425\n",
            "Iteration 46000 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.79973, b = 44.06028\n",
            "w_gradient = -0.17037, b_gradient = -0.00408\n",
            "Iteration 46200 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.80317, b = 44.06044\n",
            "w_gradient = -0.16693, b_gradient = -0.00392\n",
            "Iteration 46400 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.80654, b = 44.06059\n",
            "w_gradient = -0.16356, b_gradient = -0.00377\n",
            "Iteration 46600 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.80984, b = 44.06074\n",
            "w_gradient = -0.16026, b_gradient = -0.00362\n",
            "Iteration 46800 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.81308, b = 44.06088\n",
            "w_gradient = -0.15702, b_gradient = -0.00347\n",
            "Iteration 47000 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.81625, b = 44.06102\n",
            "w_gradient = -0.15385, b_gradient = -0.00334\n",
            "Iteration 47200 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.81936, b = 44.06115\n",
            "w_gradient = -0.15074, b_gradient = -0.00321\n",
            "Iteration 47400 : cost = 81.01\n",
            "----------------------------------------------------\n",
            "w = 8.82240, b = 44.06128\n",
            "w_gradient = -0.14770, b_gradient = -0.00308\n",
            "Iteration 47600 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.82538, b = 44.06140\n",
            "w_gradient = -0.14472, b_gradient = -0.00296\n",
            "Iteration 47800 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.82830, b = 44.06151\n",
            "w_gradient = -0.14179, b_gradient = -0.00284\n",
            "Iteration 48000 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.83117, b = 44.06163\n",
            "w_gradient = -0.13893, b_gradient = -0.00273\n",
            "Iteration 48200 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.83397, b = 44.06173\n",
            "w_gradient = -0.13612, b_gradient = -0.00262\n",
            "Iteration 48400 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.83672, b = 44.06184\n",
            "w_gradient = -0.13338, b_gradient = -0.00252\n",
            "Iteration 48600 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.83941, b = 44.06194\n",
            "w_gradient = -0.13068, b_gradient = -0.00242\n",
            "Iteration 48800 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.84205, b = 44.06203\n",
            "w_gradient = -0.12804, b_gradient = -0.00232\n",
            "Iteration 49000 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.84464, b = 44.06212\n",
            "w_gradient = -0.12546, b_gradient = -0.00223\n",
            "Iteration 49200 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.84717, b = 44.06221\n",
            "w_gradient = -0.12292, b_gradient = -0.00214\n",
            "Iteration 49400 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.84965, b = 44.06230\n",
            "w_gradient = -0.12044, b_gradient = -0.00206\n",
            "Iteration 49600 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.85209, b = 44.06238\n",
            "w_gradient = -0.11801, b_gradient = -0.00198\n",
            "Iteration 49800 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.85447, b = 44.06246\n",
            "w_gradient = -0.11563, b_gradient = -0.00190\n",
            "Iteration 50000 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.85680, b = 44.06253\n",
            "w_gradient = -0.11329, b_gradient = -0.00183\n",
            "Iteration 50200 : cost = 81.00\n",
            "----------------------------------------------------\n",
            "w = 8.85909, b = 44.06260\n",
            "w_gradient = -0.11100, b_gradient = -0.00175\n",
            "Iteration 50400 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.86133, b = 44.06267\n",
            "w_gradient = -0.10876, b_gradient = -0.00169\n",
            "Iteration 50600 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.86353, b = 44.06274\n",
            "w_gradient = -0.10656, b_gradient = -0.00162\n",
            "Iteration 50800 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.86568, b = 44.06280\n",
            "w_gradient = -0.10441, b_gradient = -0.00155\n",
            "Iteration 51000 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.86779, b = 44.06286\n",
            "w_gradient = -0.10230, b_gradient = -0.00149\n",
            "Iteration 51200 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.86985, b = 44.06292\n",
            "w_gradient = -0.10024, b_gradient = -0.00143\n",
            "Iteration 51400 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.87188, b = 44.06298\n",
            "w_gradient = -0.09821, b_gradient = -0.00138\n",
            "Iteration 51600 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.87386, b = 44.06303\n",
            "w_gradient = -0.09623, b_gradient = -0.00132\n",
            "Iteration 51800 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.87580, b = 44.06308\n",
            "w_gradient = -0.09429, b_gradient = -0.00127\n",
            "Iteration 52000 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.87771, b = 44.06313\n",
            "w_gradient = -0.09238, b_gradient = -0.00122\n",
            "Iteration 52200 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.87957, b = 44.06318\n",
            "w_gradient = -0.09052, b_gradient = -0.00117\n",
            "Iteration 52400 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.88140, b = 44.06323\n",
            "w_gradient = -0.08869, b_gradient = -0.00113\n",
            "Iteration 52600 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.88319, b = 44.06327\n",
            "w_gradient = -0.08690, b_gradient = -0.00108\n",
            "Iteration 52800 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.88495, b = 44.06332\n",
            "w_gradient = -0.08514, b_gradient = -0.00104\n",
            "Iteration 53000 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.88667, b = 44.06336\n",
            "w_gradient = -0.08342, b_gradient = -0.00100\n",
            "Iteration 53200 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.88835, b = 44.06340\n",
            "w_gradient = -0.08174, b_gradient = -0.00096\n",
            "Iteration 53400 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.89000, b = 44.06343\n",
            "w_gradient = -0.08009, b_gradient = -0.00092\n",
            "Iteration 53600 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.89162, b = 44.06347\n",
            "w_gradient = -0.07847, b_gradient = -0.00089\n",
            "Iteration 53800 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.89320, b = 44.06351\n",
            "w_gradient = -0.07689, b_gradient = -0.00085\n",
            "Iteration 54000 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.89476, b = 44.06354\n",
            "w_gradient = -0.07533, b_gradient = -0.00082\n",
            "Iteration 54200 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.89628, b = 44.06357\n",
            "w_gradient = -0.07381, b_gradient = -0.00078\n",
            "Iteration 54400 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.89777, b = 44.06360\n",
            "w_gradient = -0.07232, b_gradient = -0.00075\n",
            "Iteration 54600 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.89923, b = 44.06363\n",
            "w_gradient = -0.07086, b_gradient = -0.00072\n",
            "Iteration 54800 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.90066, b = 44.06366\n",
            "w_gradient = -0.06943, b_gradient = -0.00070\n",
            "Iteration 55000 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.90206, b = 44.06369\n",
            "w_gradient = -0.06803, b_gradient = -0.00067\n",
            "Iteration 55200 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.90343, b = 44.06371\n",
            "w_gradient = -0.06666, b_gradient = -0.00064\n",
            "Iteration 55400 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.90478, b = 44.06374\n",
            "w_gradient = -0.06531, b_gradient = -0.00062\n",
            "Iteration 55600 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.90610, b = 44.06376\n",
            "w_gradient = -0.06399, b_gradient = -0.00059\n",
            "Iteration 55800 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.90739, b = 44.06379\n",
            "w_gradient = -0.06270, b_gradient = -0.00057\n",
            "Iteration 56000 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.90866, b = 44.06381\n",
            "w_gradient = -0.06143, b_gradient = -0.00055\n",
            "Iteration 56200 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.90990, b = 44.06383\n",
            "w_gradient = -0.06019, b_gradient = -0.00053\n",
            "Iteration 56400 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91111, b = 44.06385\n",
            "w_gradient = -0.05898, b_gradient = -0.00050\n",
            "Iteration 56600 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91230, b = 44.06387\n",
            "w_gradient = -0.05778, b_gradient = -0.00048\n",
            "Iteration 56800 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91347, b = 44.06389\n",
            "w_gradient = -0.05662, b_gradient = -0.00047\n",
            "Iteration 57000 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91461, b = 44.06391\n",
            "w_gradient = -0.05547, b_gradient = -0.00045\n",
            "Iteration 57200 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91573, b = 44.06393\n",
            "w_gradient = -0.05435, b_gradient = -0.00043\n",
            "Iteration 57400 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91683, b = 44.06394\n",
            "w_gradient = -0.05326, b_gradient = -0.00041\n",
            "Iteration 57600 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91791, b = 44.06396\n",
            "w_gradient = -0.05218, b_gradient = -0.00040\n",
            "Iteration 57800 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91896, b = 44.06398\n",
            "w_gradient = -0.05113, b_gradient = -0.00038\n",
            "Iteration 58000 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.91999, b = 44.06399\n",
            "w_gradient = -0.05009, b_gradient = -0.00037\n",
            "Iteration 58200 : cost = 80.99\n",
            "----------------------------------------------------\n",
            "w = 8.92100, b = 44.06400\n",
            "w_gradient = -0.04908, b_gradient = -0.00035\n",
            "Iteration 58400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92200, b = 44.06402\n",
            "w_gradient = -0.04809, b_gradient = -0.00034\n",
            "Iteration 58600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92297, b = 44.06403\n",
            "w_gradient = -0.04712, b_gradient = -0.00032\n",
            "Iteration 58800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92392, b = 44.06404\n",
            "w_gradient = -0.04617, b_gradient = -0.00031\n",
            "Iteration 59000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92485, b = 44.06406\n",
            "w_gradient = -0.04524, b_gradient = -0.00030\n",
            "Iteration 59200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92576, b = 44.06407\n",
            "w_gradient = -0.04432, b_gradient = -0.00029\n",
            "Iteration 59400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92666, b = 44.06408\n",
            "w_gradient = -0.04343, b_gradient = -0.00028\n",
            "Iteration 59600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92754, b = 44.06409\n",
            "w_gradient = -0.04255, b_gradient = -0.00027\n",
            "Iteration 59800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92839, b = 44.06410\n",
            "w_gradient = -0.04169, b_gradient = -0.00025\n",
            "Iteration 60000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.92924, b = 44.06411\n",
            "w_gradient = -0.04085, b_gradient = -0.00024\n",
            "Iteration 60200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93006, b = 44.06412\n",
            "w_gradient = -0.04002, b_gradient = -0.00023\n",
            "Iteration 60400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93087, b = 44.06413\n",
            "w_gradient = -0.03922, b_gradient = -0.00023\n",
            "Iteration 60600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93166, b = 44.06414\n",
            "w_gradient = -0.03842, b_gradient = -0.00022\n",
            "Iteration 60800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93244, b = 44.06415\n",
            "w_gradient = -0.03765, b_gradient = -0.00021\n",
            "Iteration 61000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93320, b = 44.06416\n",
            "w_gradient = -0.03689, b_gradient = -0.00020\n",
            "Iteration 61200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93394, b = 44.06416\n",
            "w_gradient = -0.03614, b_gradient = -0.00019\n",
            "Iteration 61400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93467, b = 44.06417\n",
            "w_gradient = -0.03541, b_gradient = -0.00018\n",
            "Iteration 61600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93539, b = 44.06418\n",
            "w_gradient = -0.03470, b_gradient = -0.00018\n",
            "Iteration 61800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93609, b = 44.06419\n",
            "w_gradient = -0.03400, b_gradient = -0.00017\n",
            "Iteration 62000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93677, b = 44.06419\n",
            "w_gradient = -0.03331, b_gradient = -0.00016\n",
            "Iteration 62200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93745, b = 44.06420\n",
            "w_gradient = -0.03264, b_gradient = -0.00016\n",
            "Iteration 62400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93811, b = 44.06420\n",
            "w_gradient = -0.03198, b_gradient = -0.00015\n",
            "Iteration 62600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93875, b = 44.06421\n",
            "w_gradient = -0.03133, b_gradient = -0.00015\n",
            "Iteration 62800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.93938, b = 44.06422\n",
            "w_gradient = -0.03070, b_gradient = -0.00014\n",
            "Iteration 63000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94000, b = 44.06422\n",
            "w_gradient = -0.03008, b_gradient = -0.00013\n",
            "Iteration 63200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94061, b = 44.06423\n",
            "w_gradient = -0.02947, b_gradient = -0.00013\n",
            "Iteration 63400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94121, b = 44.06423\n",
            "w_gradient = -0.02888, b_gradient = -0.00012\n",
            "Iteration 63600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94179, b = 44.06424\n",
            "w_gradient = -0.02830, b_gradient = -0.00012\n",
            "Iteration 63800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94236, b = 44.06424\n",
            "w_gradient = -0.02772, b_gradient = -0.00011\n",
            "Iteration 64000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94292, b = 44.06425\n",
            "w_gradient = -0.02716, b_gradient = -0.00011\n",
            "Iteration 64200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94347, b = 44.06425\n",
            "w_gradient = -0.02662, b_gradient = -0.00011\n",
            "Iteration 64400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94401, b = 44.06425\n",
            "w_gradient = -0.02608, b_gradient = -0.00010\n",
            "Iteration 64600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94453, b = 44.06426\n",
            "w_gradient = -0.02555, b_gradient = -0.00010\n",
            "Iteration 64800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94505, b = 44.06426\n",
            "w_gradient = -0.02504, b_gradient = -0.00009\n",
            "Iteration 65000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94555, b = 44.06427\n",
            "w_gradient = -0.02453, b_gradient = -0.00009\n",
            "Iteration 65200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94605, b = 44.06427\n",
            "w_gradient = -0.02403, b_gradient = -0.00009\n",
            "Iteration 65400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94654, b = 44.06427\n",
            "w_gradient = -0.02355, b_gradient = -0.00008\n",
            "Iteration 65600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94701, b = 44.06428\n",
            "w_gradient = -0.02307, b_gradient = -0.00008\n",
            "Iteration 65800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94748, b = 44.06428\n",
            "w_gradient = -0.02261, b_gradient = -0.00008\n",
            "Iteration 66000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94793, b = 44.06428\n",
            "w_gradient = -0.02215, b_gradient = -0.00007\n",
            "Iteration 66200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94838, b = 44.06429\n",
            "w_gradient = -0.02170, b_gradient = -0.00007\n",
            "Iteration 66400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94882, b = 44.06429\n",
            "w_gradient = -0.02127, b_gradient = -0.00007\n",
            "Iteration 66600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94925, b = 44.06429\n",
            "w_gradient = -0.02084, b_gradient = -0.00006\n",
            "Iteration 66800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.94967, b = 44.06429\n",
            "w_gradient = -0.02041, b_gradient = -0.00006\n",
            "Iteration 67000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95008, b = 44.06430\n",
            "w_gradient = -0.02000, b_gradient = -0.00006\n",
            "Iteration 67200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95049, b = 44.06430\n",
            "w_gradient = -0.01960, b_gradient = -0.00006\n",
            "Iteration 67400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95088, b = 44.06430\n",
            "w_gradient = -0.01920, b_gradient = -0.00006\n",
            "Iteration 67600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95127, b = 44.06430\n",
            "w_gradient = -0.01882, b_gradient = -0.00005\n",
            "Iteration 67800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95165, b = 44.06430\n",
            "w_gradient = -0.01844, b_gradient = -0.00005\n",
            "Iteration 68000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95202, b = 44.06431\n",
            "w_gradient = -0.01806, b_gradient = -0.00005\n",
            "Iteration 68200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95239, b = 44.06431\n",
            "w_gradient = -0.01770, b_gradient = -0.00005\n",
            "Iteration 68400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95274, b = 44.06431\n",
            "w_gradient = -0.01734, b_gradient = -0.00005\n",
            "Iteration 68600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95309, b = 44.06431\n",
            "w_gradient = -0.01699, b_gradient = -0.00004\n",
            "Iteration 68800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95344, b = 44.06431\n",
            "w_gradient = -0.01665, b_gradient = -0.00004\n",
            "Iteration 69000 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95377, b = 44.06432\n",
            "w_gradient = -0.01631, b_gradient = -0.00004\n",
            "Iteration 69200 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95410, b = 44.06432\n",
            "w_gradient = -0.01598, b_gradient = -0.00004\n",
            "Iteration 69400 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95442, b = 44.06432\n",
            "w_gradient = -0.01566, b_gradient = -0.00004\n",
            "Iteration 69600 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95474, b = 44.06432\n",
            "w_gradient = -0.01534, b_gradient = -0.00004\n",
            "Iteration 69800 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "w = 8.95505, b = 44.06432\n",
            "w_gradient = -0.01503, b_gradient = -0.00003\n",
            "Iteration 69999 : cost = 80.98\n",
            "----------------------------------------------------\n",
            "Final weights -> w: 8.95505, b: 44.06432\n",
            "---------------------------------\n",
            "MAPE: 17.66891%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other solution(Better performance)"
      ],
      "metadata": {
        "id": "fG-K89NhyZm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y = w3x^4 + w2x^3 + w1x^2 + wx + b\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# 檔案路徑\n",
        "training_dataroot = 'lab1_basic_training.csv'  # 訓練資料檔案\n",
        "testing_dataroot = 'lab1_basic_testing.csv'    # 測試資料檔案\n",
        "output_dataroot = 'lab1_basic.csv'             # 預測結果輸出檔案\n",
        "\n",
        "# 讀取CSV檔案\n",
        "training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
        "testing_datalist = pd.read_csv(testing_dataroot).to_numpy()\n",
        "\n",
        "def SplitData(data, split_ratio):\n",
        "    \"\"\"\n",
        "    根據指定的比例將資料集分割為訓練集和驗證集。\n",
        "\n",
        "    參數:\n",
        "    - data (numpy.ndarray): 要分割的資料集，每行代表一個數據點。\n",
        "    - split_ratio (float): 用於訓練的資料比例（例如，0.9表示90%的資料用於訓練）。\n",
        "\n",
        "    回傳:\n",
        "    - training_data (numpy.ndarray): 訓練集。\n",
        "    - validation_data (numpy.ndarray): 驗證集。\n",
        "    \"\"\"\n",
        "    num_training_samples = int(len(data) * split_ratio)\n",
        "    training_data = data[:num_training_samples]\n",
        "    validation_data = data[num_training_samples:]\n",
        "    return training_data, validation_data\n",
        "\n",
        "def PreprocessData(data):\n",
        "    \"\"\"\n",
        "    預處理資料集，刪除包含遺失值或極端值的整行資料。\n",
        "\n",
        "    參數:\n",
        "    - data (numpy.ndarray): 要預處理的資料集，每行代表一個數據點。\n",
        "\n",
        "    回傳:\n",
        "    - preprocessedData (numpy.ndarray): 預處理後的資料集。\n",
        "    \"\"\"\n",
        "    # 將資料轉換為 DataFrame 便於處理\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 步驟 1: 刪除包含遺失值的行\n",
        "    initial_shape = df.shape\n",
        "    df.dropna(inplace=True)\n",
        "    after_dropna_shape = df.shape\n",
        "    print(f\"Deleted {initial_shape[0] - after_dropna_shape[0]} rows containing missing values.\")\n",
        "\n",
        "    # 步驟 2: 刪除包含任何極端值的行\n",
        "    outlier_indices = set()\n",
        "    for col in df.columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        # 找出該欄位的極端值索引\n",
        "        col_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
        "        outlier_indices.update(col_outliers)\n",
        "        print(f\"Column {col}: Q1 = {Q1:.2f}, Q3 = {Q3:.2f}, IQR = {IQR:.2f}\")\n",
        "        print(f\"Outlier range for column {col}: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "        print(f\"Number of outliers in column {col}: {len(col_outliers)}\")\n",
        "\n",
        "    # 刪除所有包含任何極端值的行\n",
        "    df_before_outlier_drop = df.shape\n",
        "    df.drop(index=outlier_indices, inplace=True)\n",
        "    df_after_outlier_drop = df.shape\n",
        "    print(f\"Deleted {df_before_outlier_drop[0] - df_after_outlier_drop[0]} rows containing outliers.\")\n",
        "\n",
        "    # 最終預處理後的資料\n",
        "    preprocessedData = df.to_numpy()\n",
        "\n",
        "    print(f\"Preprocessed data shape: {preprocessedData.shape}\")\n",
        "\n",
        "    return preprocessedData\n",
        "\n",
        "def Regression(dataset):\n",
        "    \"\"\"\n",
        "    使用矩陣求解法進行四次回歸並返回權重 w3, w2, w1, w 和偏差 b。\n",
        "    \"\"\"\n",
        "    X = dataset[:, 0].reshape(-1, 1)  # 特徵\n",
        "    y = dataset[:, 1]  # 目標變量\n",
        "\n",
        "    # 增加 x, x^2, x^3 和 x^4\n",
        "    X_squared = X**2\n",
        "    X_cubed = X**3\n",
        "    X_fourth = X**4\n",
        "\n",
        "    # 增加一列全是1的數據，作為截距項\n",
        "    X_with_intercept = np.column_stack((np.ones(X.shape[0]), X_fourth, X_cubed, X_squared, X))\n",
        "\n",
        "    # 使用公式 w = (X.T * X)^(-1) * X.T * y\n",
        "    try:\n",
        "        w = np.linalg.inv(X_with_intercept.T.dot(X_with_intercept)).dot(X_with_intercept.T).dot(y)\n",
        "    except np.linalg.LinAlgError:\n",
        "        # 若 X.T * X 不可逆，使用偽逆\n",
        "        w = np.linalg.pinv(X_with_intercept.T.dot(X_with_intercept)).dot(X_with_intercept.T).dot(y)\n",
        "        print(\"Matrix is singular, using pseudo-inverse instead.\")\n",
        "\n",
        "    # 權重與偏差\n",
        "    b = w[0]     # 截距項\n",
        "    w3 = w[1]    # 四次項權重\n",
        "    w2 = w[2]    # 三次項權重\n",
        "    w1 = w[3]    # 二次項權重\n",
        "    w = w[4]     # 一次項權重\n",
        "\n",
        "    return w3, w2, w1, w, b\n",
        "\n",
        "def MakePrediction(w3, w2, w1, w, b, test_dataset):\n",
        "    \"\"\"\n",
        "    使用 w3, w2, w1, w 和 b 進行預測，這裡使用 x^4, x^3, x^2 和 x。\n",
        "    \"\"\"\n",
        "    X = test_dataset\n",
        "    predictions = w3 * (X**4) + w2 * (X**3) + w1 * (X**2) + w * X + b\n",
        "    return predictions\n",
        "\n",
        "def MAPE(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    計算平均絕對百分比誤差（MAPE）。\n",
        "\n",
        "    參數:\n",
        "    - y_true (numpy.ndarray): 真實目標值。\n",
        "    - y_pred (numpy.ndarray): 預測目標值。\n",
        "\n",
        "    回傳:\n",
        "    - mape (float): MAPE值（百分比）。\n",
        "    \"\"\"\n",
        "    y_true_nonzero_mean = np.mean(y_true[y_true != 0])\n",
        "    y_true_safe = np.where(y_true == 0, y_true_nonzero_mean, y_true)\n",
        "    y_diff = np.abs(y_true_safe - y_pred) / y_true_safe\n",
        "    mape = (y_diff.sum() / len(y_true)) * 100\n",
        "    return mape\n",
        "\n",
        "# 資料處理步驟\n",
        "\n",
        "# 1. 分割資料\n",
        "split_ratio = 0.9\n",
        "training_data, validation_data = SplitData(training_datalist, split_ratio)\n",
        "\n",
        "# 2. 預處理資料\n",
        "training_data = PreprocessData(training_data)\n",
        "validation_data = PreprocessData(validation_data)\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "print(\"Training data shape:\", training_data.shape)\n",
        "print(\"Validation data shape:\", validation_data.shape)\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# 分離特徵與標籤\n",
        "X_train = training_data[:, :-1]\n",
        "y_train = training_data[:, -1]\n",
        "X_val = validation_data[:, :-1]\n",
        "y_val = validation_data[:, -1]\n",
        "\n",
        "# 3. 訓練回歸模型\n",
        "w3_final, w2_final, w1_final, w_final, b_final = Regression(training_data)\n",
        "print(f\"w3 = {w3_final:.5f}, w2 = {w2_final:.5f}, w1 = {w1_final:.5f}, w = {w_final:.5f}, b = {b_final:.5f}\")\n",
        "\n",
        "# 4. 預測驗證集的結果並計算 MAPE\n",
        "prediction_data = MakePrediction(w3_final, w2_final, w1_final, w_final, b_final, validation_data[:, 0])\n",
        "y_val = validation_data[:, 1]\n",
        "mape = MAPE(y_val, prediction_data)\n",
        "print(f\"MAPE: {mape:.5f}%\")\n",
        "\n",
        "# 5. 預測測試集並保存結果\n",
        "# 假設測試集只有特徵，無標籤\n",
        "testing_X = testing_datalist[:, 0]\n",
        "prediction_ans = MakePrediction(w3_final, w2_final, w1_final, w_final, b_final, testing_X)\n",
        "output_datalist = prediction_ans.reshape(-1, 1)\n",
        "\n",
        "# 將預測結果寫入CSV檔案\n",
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Id', 'gripForce'])\n",
        "    for i, prediction in enumerate(output_datalist):\n",
        "        writer.writerow([i, prediction[0]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hycPgOA9yd_Z",
        "outputId": "d2f36a64-14b4-4826-b7c3-8eca50e92bfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted 70 rows containing missing values.\n",
            "Column 0: Q1 = 57.30, Q3 = 75.70, IQR = 18.40\n",
            "Outlier range for column 0: [29.70, 103.30]\n",
            "Number of outliers in column 0: 560\n",
            "Column 1: Q1 = 33.20, Q3 = 55.40, IQR = 22.20\n",
            "Outlier range for column 1: [-0.10, 88.70]\n",
            "Number of outliers in column 1: 446\n",
            "Deleted 982 rows containing outliers.\n",
            "Preprocessed data shape: (7948, 2)\n",
            "Deleted 3 rows containing missing values.\n",
            "Column 0: Q1 = 58.70, Q3 = 76.10, IQR = 17.40\n",
            "Outlier range for column 0: [32.60, 102.20]\n",
            "Number of outliers in column 0: 63\n",
            "Column 1: Q1 = 33.80, Q3 = 56.00, IQR = 22.20\n",
            "Outlier range for column 1: [0.50, 89.30]\n",
            "Number of outliers in column 1: 50\n",
            "Deleted 108 rows containing outliers.\n",
            "Preprocessed data shape: (889, 2)\n",
            "-----------------------------------------------\n",
            "Training data shape: (7948, 2)\n",
            "Validation data shape: (889, 2)\n",
            "-----------------------------------------------\n",
            "w3 = 0.00002, w2 = -0.00614, w1 = 0.66926, w = -30.36000, b = 513.97924\n",
            "MAPE: 17.50249%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Advanced Part (45%)\n",
        "In the second part, you need to implement regression differently from the basic part to improve your grip force predictions. You must use more than two features.\n",
        "\n",
        "You can choose either matrix inversion or gradient descent for this part\n",
        "\n",
        "We have provided `lab1_advanced_training.csv` for your training\n",
        "\n",
        "> Notice: Be cautious of the \"gender\" attribute, as it is represented by \"F\"/\"M\" rather than a numerical value.\n",
        "\n",
        "Please save the prediction result in a CSV file and submit it to Kaggle"
      ],
      "metadata": {
        "id": "V1O2l8d2E3he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent"
      ],
      "metadata": {
        "id": "WmAQ7Jf8zhmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File paths\n",
        "training_dataroot = 'lab1_advanced_training.csv'\n",
        "testing_dataroot = 'lab1_advanced_testing.csv'\n",
        "output_dataroot = 'lab1_advanced.csv'\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path).to_numpy()\n",
        "    data = Convert_gender_to_numeric(data)\n",
        "    data = PreprocessData(data)\n",
        "    return data\n",
        "\n",
        "def Convert_gender_to_numeric(data):\n",
        "    df = pd.DataFrame(data)\n",
        "    gender_col_index = 1\n",
        "    df[gender_col_index] = df[gender_col_index].map({'F': 0, 'M': 1})\n",
        "    df = df.dropna()\n",
        "    return df.to_numpy()\n",
        "\n",
        "def PreprocessData(data):\n",
        "    try:\n",
        "        data = data.astype(float)\n",
        "    except ValueError as e:\n",
        "        print(\"Error converting data to float:\", e)\n",
        "        for col in range(data.shape[1]):\n",
        "            try:\n",
        "                data[:, col] = data[:, col].astype(float)\n",
        "            except ValueError:\n",
        "                print(f\"Column {col} cannot be converted to float and will be skipped.\")\n",
        "                data = np.delete(data, col, axis=1)\n",
        "    data = data[~np.isnan(data).any(axis=1)]\n",
        "    return data\n",
        "\n",
        "def PreprocessData2(data):\n",
        "    rows_to_delete = []\n",
        "    for col in range(data.shape[1]):\n",
        "        col_data = data[:, col]\n",
        "        if np.issubdtype(col_data.dtype, np.number):\n",
        "            Q1 = np.percentile(col_data, 25)\n",
        "            Q3 = np.percentile(col_data, 75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            rows_to_delete.extend(np.where((col_data < lower_bound) | (col_data > upper_bound))[0])\n",
        "    data = np.delete(data, list(set(rows_to_delete)), axis=0)\n",
        "    return data\n",
        "\n",
        "def SplitByGender(data):\n",
        "    gender_col_index = 1\n",
        "    male_data = data[data[:, gender_col_index] == 1]\n",
        "    female_data = data[data[:, gender_col_index] == 0]\n",
        "    return male_data, female_data\n",
        "\n",
        "def compute_gradient(x, y, w, b):\n",
        "    m = len(y)\n",
        "    y_pred = (x @ w) + b\n",
        "    w_gradient = (x.T @ (y_pred - y)) / m\n",
        "    b_gradient = (y_pred - y).mean()\n",
        "    return w_gradient, b_gradient\n",
        "\n",
        "def compute_cost(x, y, w, b):\n",
        "    m = len(y)\n",
        "    y_pred = (x @ w) + b\n",
        "    cost = ((y - y_pred) ** 2).mean()\n",
        "    return cost\n",
        "\n",
        "def Regression(x, y, learning_rate_w, learning_rate_b, num_iterations=20000):\n",
        "    w = np.zeros(x.shape[1])\n",
        "    b = 0.0\n",
        "    cost_hist = []\n",
        "    w_hist = []\n",
        "    b_hist = []\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        cost = compute_cost(x, y, w, b)\n",
        "        w_gradient, b_gradient = compute_gradient(x, y, w, b)\n",
        "        w -= learning_rate_w * w_gradient\n",
        "        b -= learning_rate_b * b_gradient\n",
        "\n",
        "        w_hist.append(w.copy())\n",
        "        b_hist.append(b)\n",
        "        cost_hist.append(cost)\n",
        "\n",
        "        if iteration % 500 == 0 or iteration == num_iterations - 1:\n",
        "            w_str = ', '.join([f'{wi:.5f}' for wi in w])\n",
        "            print('----------------------------------------------------')\n",
        "            print(f\"w = [{w_str}], b = {b:.5f}\")\n",
        "            print(f\"w_gradient = {w_gradient}, b_gradient = {b_gradient:.5f}\")\n",
        "            print(f\"Iteration {iteration:5} : cost = {cost:.2f}\")\n",
        "            print('----------------------------------------------------')\n",
        "\n",
        "    return w, b, cost_hist, w_hist, b_hist\n",
        "\n",
        "def calculate_mape(y_true, y_pred):\n",
        "    epsilon = 1e-10\n",
        "    y_true = np.where(y_true == 0, epsilon, y_true)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    return mape\n",
        "\n",
        "def predict_validation(w, b, validation_data):\n",
        "    x_val = validation_data[:, :-1]\n",
        "    y_true = validation_data[:, -1]\n",
        "    y_pred = (x_val @ w) + b\n",
        "    return y_pred, y_true\n",
        "\n",
        "def MakePrediction(w_m, b_m, scaler_m, w_f, b_f, scaler_f, test_dataset):\n",
        "    y_pred = []\n",
        "    for row in test_dataset:\n",
        "        gender = row[1]\n",
        "        features = row[0:]  # Features start after gender\n",
        "        if gender == 1:\n",
        "            features = scaler_m.transform([features])[0]\n",
        "            prediction = np.dot(features, w_m) + b_m\n",
        "        elif gender == 0:\n",
        "            features = scaler_f.transform([features])[0]\n",
        "            prediction = np.dot(features, w_f) + b_f\n",
        "        else:\n",
        "            prediction = np.nan\n",
        "        y_pred.append(prediction)\n",
        "    return np.array(y_pred)\n",
        "\n",
        "def save_predictions(predictions, output_file):\n",
        "    with open(output_file, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Id', 'gripForce'])\n",
        "        for i, prediction in enumerate(predictions):\n",
        "            writer.writerow([i, prediction])\n",
        "    print(f\"Predictions saved to {output_file}.\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = load_and_preprocess_data(training_dataroot)\n",
        "test_data = load_and_preprocess_data(testing_dataroot)\n",
        "\n",
        "# Split training data\n",
        "split_ratio = 0.8\n",
        "train_data, valid_data = train_test_split(train_data, test_size=1-split_ratio, random_state=42)\n",
        "\n",
        "# Split by gender\n",
        "train_m, train_f = SplitByGender(train_data)\n",
        "valid_m, valid_f = SplitByGender(valid_data)\n",
        "\n",
        "# Remove extreme values\n",
        "train_m = PreprocessData2(train_m)\n",
        "train_f = PreprocessData2(train_f)\n",
        "valid_m = PreprocessData2(valid_m)\n",
        "valid_f = PreprocessData2(valid_f)\n",
        "\n",
        "# Standardize data\n",
        "scaler_m = StandardScaler()\n",
        "scaler_f = StandardScaler()\n",
        "train_m[:, :-1] = scaler_m.fit_transform(train_m[:, :-1])\n",
        "train_f[:, :-1] = scaler_f.fit_transform(train_f[:, :-1])\n",
        "valid_m[:, :-1] = scaler_m.transform(valid_m[:, :-1])\n",
        "valid_f[:, :-1] = scaler_f.transform(valid_f[:, :-1])\n",
        "\n",
        "# Train models\n",
        "learning_rate_w_m = np.array([0.001] * (train_m.shape[1] - 1))\n",
        "learning_rate_w_f = np.array([0.001] * (train_f.shape[1] - 1))\n",
        "learning_rate_b = 0.001\n",
        "\n",
        "w_m, b_m, _, _, _ = Regression(train_m[:, :-1], train_m[:, -1], learning_rate_w_m, learning_rate_b)\n",
        "w_f, b_f, _, _, _ = Regression(train_f[:, :-1], train_f[:, -1], learning_rate_w_f, learning_rate_b)\n",
        "\n",
        "# Validate models\n",
        "y_pred_m, y_true_m = predict_validation(w_m, b_m, valid_m)\n",
        "y_pred_f, y_true_f = predict_validation(w_f, b_f, valid_f)\n",
        "\n",
        "print(f\"MAPE on male validation dataset: {calculate_mape(y_true_m, y_pred_m):.2f}%\")\n",
        "print(f\"MAPE on female validation dataset: {calculate_mape(y_true_f, y_pred_f):.2f}%\")\n",
        "\n",
        "# Predict and save results\n",
        "test_data = PreprocessData(test_data)\n",
        "output_datalist = MakePrediction(w_m, b_m, scaler_m, w_f, b_f, scaler_f, test_data)\n",
        "save_predictions(output_datalist, output_dataroot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG_dzIW9zhF7",
        "outputId": "17891d4d-c775-4605-9b9b-ad82730f48cf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------\n",
            "w = [-0.00166, 0.00000, 0.00294, 0.00320, -0.00182, 0.00029, 0.00055], b = 0.05207\n",
            "w_gradient = [ 1.66204081  0.         -2.93588116 -3.19870148  1.82261437 -0.28570279\n",
            " -0.54589325], b_gradient = -52.07306\n",
            "Iteration     0 : cost = 2779.16\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.51236, 0.00000, 0.93822, 1.18363, -0.76860, 0.11881, 0.21131], b = 20.52863\n",
            "w_gradient = [ 0.54817698  0.         -1.06628584 -1.72528463  1.2888508  -0.19149204\n",
            " -0.31379735], b_gradient = -31.57600\n",
            "Iteration   500 : cost = 1051.27\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.65835, 0.00000, 1.24085, 1.86002, -1.32288, 0.19609, 0.32826], b = 32.94518\n",
            "w_gradient = [ 0.10616048  0.         -0.25835111 -1.06667352  0.95344389 -0.12254564\n",
            " -0.16609727], b_gradient = -19.14702\n",
            "Iteration  1000 : cost = 417.12\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.66386, 0.00000, 1.27183, 2.30640, -1.74068, 0.24566, 0.38695], b = 40.47431\n",
            "w_gradient = [-0.05508108  0.          0.08431507 -0.75577356  0.73273466 -0.07954836\n",
            " -0.07655846], b_gradient = -11.61035\n",
            "Iteration  1500 : cost = 183.77\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.62175, 0.00000, 1.18973, 2.63982, -2.06689, 0.27866, 0.41086], b = 45.03982\n",
            "w_gradient = [-0.1018588   0.          0.22190114 -0.59393587  0.58139422 -0.05490628\n",
            " -0.02394189], b_gradient = -7.04027\n",
            "Iteration  2000 : cost = 97.61\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.56914, 0.00000, 1.06464, 2.91080, -2.32914, 0.30241, 0.41458], b = 47.80825\n",
            "w_gradient = [-0.10441596  0.          0.26867154 -0.4971743   0.47355457 -0.0415511\n",
            "  0.00611901], b_gradient = -4.26907\n",
            "Iteration  2500 : cost = 65.63\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.51976, 0.00000, 0.92759, 3.14177, -2.54501, 0.32125, 0.40695], b = 49.48697\n",
            "w_gradient = [-0.09193939  0.          0.27525654 -0.43016181  0.39387775 -0.03457704\n",
            "  0.02266348], b_gradient = -2.58868\n",
            "Iteration  3000 : cost = 53.62\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.47773, 0.00000, 0.79224, 3.34335, -2.72606, 0.33754, 0.39323], b = 50.50491\n",
            "w_gradient = [-0.07613376  0.          0.26435177 -0.37805875  0.33300349 -0.0309601\n",
            "  0.03119131], b_gradient = -1.56972\n",
            "Iteration  3500 : cost = 49.00\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.44346, 0.00000, 0.66442, 3.52120, -2.88011, 0.35248, 0.37653], b = 51.12216\n",
            "w_gradient = [-0.06133636  0.          0.24627385 -0.33457066  0.28508201 -0.02896468\n",
            "  0.03499756], b_gradient = -0.95184\n",
            "Iteration  4000 : cost = 47.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.41602, 0.00000, 0.54637, 3.67884, -3.01263, 0.36661, 0.35868], b = 51.49645\n",
            "w_gradient = [-0.04885154  0.          0.22578113 -0.29692974  0.24637532 -0.02765225\n",
            "  0.03604483], b_gradient = -0.57718\n",
            "Iteration  4500 : cost = 46.35\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.39421, 0.00000, 0.43869, 3.81884, -3.12757, 0.38016, 0.34075], b = 51.72342\n",
            "w_gradient = [-0.03879714  0.          0.20509138 -0.26380965  0.21443934 -0.02654892\n",
            "  0.03550153], b_gradient = -0.34999\n",
            "Iteration  5000 : cost = 45.95\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.37688, 0.00000, 0.34116, 3.94325, -3.22789, 0.39316, 0.32333], b = 51.86104\n",
            "w_gradient = [-0.03087936  0.          0.18523412 -0.23447178  0.18763574 -0.02543935\n",
            "  0.03406503], b_gradient = -0.21223\n",
            "Iteration  5500 : cost = 45.73\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.36305, 0.00000, 0.25325, 4.05383, -3.31585, 0.40558, 0.30676], b = 51.94450\n",
            "w_gradient = [-0.02470731  0.          0.16666765 -0.20842229  0.16483606 -0.02424537\n",
            "  0.03215311], b_gradient = -0.12869\n",
            "Iteration  6000 : cost = 45.59\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.35194, 0.00000, 0.17427, 4.15213, -3.39324, 0.41739, 0.29122], b = 51.99510\n",
            "w_gradient = [-0.01991154  0.          0.14956751 -0.18527748  0.14524019 -0.02295696\n",
            "  0.03001698], b_gradient = -0.07803\n",
            "Iteration  6500 : cost = 45.49\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.34296, 0.00000, 0.10345, 4.23952, -3.46151, 0.42853, 0.27676], b = 52.02579\n",
            "w_gradient = [-0.01618098  0.          0.13396569 -0.16471227  0.12826375 -0.02159444\n",
            "  0.0278079 ], b_gradient = -0.04732\n",
            "Iteration  7000 : cost = 45.42\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.33563, 0.00000, 0.04007, 4.31721, -3.52185, 0.43897, 0.26341], b = 52.04439\n",
            "w_gradient = [-0.01326727  0.          0.11982032 -0.14644058  0.11346725 -0.0201885\n",
            "  0.02561665], b_gradient = -0.02869\n",
            "Iteration  7500 : cost = 45.36\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.32959, 0.00000, -0.01658, 4.38628, -3.57527, 0.44871, 0.25113], b = 52.05567\n",
            "w_gradient = [-0.01097761  0.          0.10705217 -0.13020752  0.10051104 -0.01877031\n",
            "  0.02349703], b_gradient = -0.01740\n",
            "Iteration  8000 : cost = 45.31\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.32458, 0.00000, -0.06718, 4.44770, -3.62261, 0.45774, 0.23990], b = 52.06252\n",
            "w_gradient = [-0.00916461  0.          0.09556437 -0.11578546  0.08912599 -0.01736714\n",
            "  0.02147996], b_gradient = -0.01055\n",
            "Iteration  8500 : cost = 45.28\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.32037, 0.00000, -0.11233, 4.50232, -3.66461, 0.46608, 0.22964], b = 52.06667\n",
            "w_gradient = [-0.00771655  0.          0.08525359 -0.10297148  0.07909427 -0.01600083\n",
            "  0.01958204], b_gradient = -0.00640\n",
            "Iteration  9000 : cost = 45.25\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.31681, 0.00000, -0.15260, 4.55090, -3.70189, 0.47375, 0.22029], b = 52.06918\n",
            "w_gradient = [-0.00654909  0.          0.07601641 -0.091585    0.07023633 -0.01468772\n",
            "  0.0178108 ], b_gradient = -0.00388\n",
            "Iteration  9500 : cost = 45.23\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.31379, 0.00000, -0.18850, 4.59411, -3.73501, 0.48078, 0.21181], b = 52.07071\n",
            "w_gradient = [-0.00559867  0.          0.06775292 -0.08146561  0.06240193 -0.0134392\n",
            "  0.01616803], b_gradient = -0.00235\n",
            "Iteration 10000 : cost = 45.21\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.31119, 0.00000, -0.22050, 4.63254, -3.76443, 0.48720, 0.20411], b = 52.07163\n",
            "w_gradient = [-0.00481733  0.          0.06036879 -0.07247099  0.0554638  -0.01226254\n",
            "  0.01465182], b_gradient = -0.00143\n",
            "Iteration 10500 : cost = 45.20\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.30895, 0.00000, -0.24900, 4.66673, -3.79059, 0.49306, 0.19713], b = 52.07219\n",
            "w_gradient = [-0.0041688   0.          0.05377625 -0.06447494  0.04931312 -0.01116169\n",
            "  0.01325794], b_gradient = -0.00086\n",
            "Iteration 11000 : cost = 45.19\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.30700, 0.00000, -0.27438, 4.69716, -3.81386, 0.49838, 0.19083], b = 52.07253\n",
            "w_gradient = [-0.00362555  0.          0.04789454 -0.05736559  0.04385605 -0.01013803\n",
            "  0.01198072], b_gradient = -0.00052\n",
            "Iteration 11500 : cost = 45.18\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.30531, 0.00000, -0.29699, 4.72422, -3.83455, 0.50321, 0.18514], b = 52.07274\n",
            "w_gradient = [-0.00316657  0.          0.04264987 -0.05104376  0.0390112  -0.00919101\n",
            "  0.01081366], b_gradient = -0.00032\n",
            "Iteration 12000 : cost = 45.17\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.30383, 0.00000, -0.31712, 4.74831, -3.85295, 0.50758, 0.18000], b = 52.07286\n",
            "w_gradient = [-0.00277575  0.          0.03797526 -0.04542152  0.03470763 -0.00831869\n",
            "  0.00974982], b_gradient = -0.00019\n",
            "Iteration 12500 : cost = 45.16\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.30252, 0.00000, -0.33505, 4.76974, -3.86933, 0.51154, 0.17537], b = 52.07294\n",
            "w_gradient = [-0.0024406   0.          0.03381016 -0.04042088  0.03088322 -0.00751815\n",
            "  0.00878209], b_gradient = -0.00012\n",
            "Iteration 13000 : cost = 45.16\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.30138, 0.00000, -0.35101, 4.78882, -3.88390, 0.51511, 0.17120], b = 52.07299\n",
            "w_gradient = [-0.0021514   0.          0.03010001 -0.03597266  0.02748346 -0.00678582\n",
            "  0.00790342], b_gradient = -0.00007\n",
            "Iteration 13500 : cost = 45.16\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.30037, 0.00000, -0.36521, 4.80579, -3.89687, 0.51833, 0.16746], b = 52.07301\n",
            "w_gradient = [-0.00190048  0.          0.0267958  -0.03201548  0.02446032 -0.0061177\n",
            "  0.00710693], b_gradient = -0.00004\n",
            "Iteration 14000 : cost = 45.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29947, 0.00000, -0.37786, 4.82090, -3.90842, 0.52123, 0.16409], b = 52.07303\n",
            "w_gradient = [-0.00168175  0.          0.02385358 -0.02849483  0.02177147 -0.00550963\n",
            "  0.006386  ], b_gradient = -0.00003\n",
            "Iteration 14500 : cost = 45.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29868, 0.00000, -0.38912, 4.83435, -3.91869, 0.52385, 0.16106], b = 52.07304\n",
            "w_gradient = [-0.00149031  0.          0.02123399 -0.02536232  0.01937949 -0.00495736\n",
            "  0.00573432], b_gradient = -0.00002\n",
            "Iteration 15000 : cost = 45.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29798, 0.00000, -0.39914, 4.84632, -3.92784, 0.52620, 0.15834], b = 52.07305\n",
            "w_gradient = [-0.0013222   0.          0.01890188 -0.02257497  0.01725126 -0.00445669\n",
            "  0.00514596], b_gradient = -0.00001\n",
            "Iteration 15500 : cost = 45.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29735, 0.00000, -0.40806, 4.85697, -3.93598, 0.52831, 0.15590], b = 52.07305\n",
            "w_gradient = [-0.00117416  0.          0.01682583 -0.0200946   0.01535747 -0.00400353\n",
            "  0.00461533], b_gradient = -0.00001\n",
            "Iteration 16000 : cost = 45.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29680, 0.00000, -0.41600, 4.86646, -3.94323, 0.53021, 0.15372], b = 52.07305\n",
            "w_gradient = [-0.00104348  0.          0.01497781 -0.01788728  0.0136721  -0.00359397\n",
            "  0.00413725], b_gradient = -0.00000\n",
            "Iteration 16500 : cost = 45.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29631, 0.00000, -0.42307, 4.87490, -3.94968, 0.53191, 0.15176], b = 52.07305\n",
            "w_gradient = [-0.0009279   0.          0.01333282 -0.01592285  0.01217208 -0.00322428\n",
            "  0.0037069 ], b_gradient = -0.00000\n",
            "Iteration 17000 : cost = 45.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29587, 0.00000, -0.42936, 4.88241, -3.95543, 0.53344, 0.15000], b = 52.07305\n",
            "w_gradient = [-0.00082553  0.          0.0118686  -0.01417451  0.01083693 -0.00289096\n",
            "  0.00331983], b_gradient = -0.00000\n",
            "Iteration 17500 : cost = 45.15\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29548, 0.00000, -0.43496, 4.88910, -3.96054, 0.53481, 0.14843], b = 52.07306\n",
            "w_gradient = [-0.00073472  0.          0.01056528 -0.01261842  0.00964845 -0.00259074\n",
            "  0.00297196], b_gradient = -0.00000\n",
            "Iteration 18000 : cost = 45.14\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29513, 0.00000, -0.43995, 4.89506, -3.96510, 0.53604, 0.14703], b = 52.07306\n",
            "w_gradient = [-0.0006541   0.          0.00940519 -0.01123339  0.00859049 -0.0023206\n",
            "  0.00265953], b_gradient = -0.00000\n",
            "Iteration 18500 : cost = 45.14\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29483, 0.00000, -0.44439, 4.90036, -3.96915, 0.53713, 0.14577], b = 52.07306\n",
            "w_gradient = [-0.00058247  0.          0.00837258 -0.01000058  0.00764866 -0.00207771\n",
            "  0.00237911], b_gradient = -0.00000\n",
            "Iteration 19000 : cost = 45.14\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29455, 0.00000, -0.44834, 4.90508, -3.97276, 0.53812, 0.14464], b = 52.07306\n",
            "w_gradient = [-0.00051876  0.          0.00745345 -0.00890322  0.00681018 -0.0018595\n",
            "  0.00212756], b_gradient = -0.00000\n",
            "Iteration 19500 : cost = 45.14\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.29431, 0.00000, -0.45185, 4.90928, -3.97597, 0.53900, 0.14364], b = 52.07306\n",
            "w_gradient = [-0.0004622   0.          0.00663685 -0.00792824  0.00606511 -0.00166396\n",
            "  0.00190247], b_gradient = -0.00000\n",
            "Iteration 19999 : cost = 45.14\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.00127, 0.00000, 0.00203, 0.00177, -0.00107, 0.00044, 0.00023], b = 0.03093\n",
            "w_gradient = [ 1.2651882   0.         -2.02656088 -1.77325324  1.07443815 -0.44231076\n",
            " -0.22914417], b_gradient = -30.92832\n",
            "Iteration     0 : cost = 984.08\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.42331, 0.00000, 0.67764, 0.68776, -0.43993, 0.16858, 0.11174], b = 12.19279\n",
            "w_gradient = [ 0.52889071  0.         -0.83080712 -1.05151078  0.72152451 -0.24161291\n",
            " -0.20200371], b_gradient = -18.75428\n",
            "Iteration   500 : cost = 373.65\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.59900, 0.00000, 0.93706, 1.10986, -0.74875, 0.25402, 0.19695], b = 19.56749\n",
            "w_gradient = [ 0.21902949  0.         -0.27846901 -0.67765824  0.53140456 -0.1118345\n",
            " -0.13915316], b_gradient = -11.37220\n",
            "Iteration  1000 : cost = 149.66\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.67176, 0.00000, 1.00458, 1.39372, -0.98327, 0.28996, 0.25298], b = 24.03935\n",
            "w_gradient = [ 0.0913103   0.         -0.02512615 -0.4781192   0.41535947 -0.03976897\n",
            " -0.0880409 ], b_gradient = -6.89586\n",
            "Iteration  1500 : cost = 67.28\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.70287, 0.00000, 0.98499, 1.60207, -1.17023, 0.29972, 0.28781], b = 26.75099\n",
            "w_gradient = [ 0.04127033  0.          0.0876594  -0.36544208  0.33725729 -0.00384575\n",
            " -0.0538913 ], b_gradient = -4.18151\n",
            "Iteration  2000 : cost = 36.88\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.71829, 0.00000, 0.92774, 1.76624, -1.32404, 0.29710, 0.30899], b = 28.39527\n",
            "w_gradient = [ 0.02372932  0.          0.13384733 -0.29645084  0.28086344  0.01191901\n",
            " -0.032588  ], b_gradient = -2.53558\n",
            "Iteration  2500 : cost = 25.61\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.72868, 0.00000, 0.85629, 1.90217, -1.45329, 0.28950, 0.32175], b = 29.39233\n",
            "w_gradient = [ 0.01911477  0.          0.14841501 -0.25004789  0.23805242  0.0172779\n",
            " -0.01956302], b_gradient = -1.53752\n",
            "Iteration  3000 : cost = 21.38\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.73813, 0.00000, 0.78175, 2.01827, -1.56356, 0.28064, 0.32937], b = 29.99693\n",
            "w_gradient = [ 0.01905955  0.          0.14809929 -0.21593427  0.20433612  0.01766021\n",
            " -0.01155714], b_gradient = -0.93232\n",
            "Iteration  3500 : cost = 19.76\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.74792, 0.00000, 0.70931, 2.11926, -1.65866, 0.27219, 0.33380], b = 30.36354\n",
            "w_gradient = [ 0.02012235  0.          0.14099321 -0.18903086  0.17703335  0.01591461\n",
            " -0.00653674], b_gradient = -0.56534\n",
            "Iteration  4000 : cost = 19.11\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.75822, 0.00000, 0.64123, 2.20804, -1.74135, 0.26484, 0.33620], b = 30.58585\n",
            "w_gradient = [ 0.02101118  0.          0.13104384 -0.16676808  0.15444507  0.01345995\n",
            " -0.00330145], b_gradient = -0.34281\n",
            "Iteration  4500 : cost = 18.83\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.76884, 0.00000, 0.57842, 2.28655, -1.81368, 0.25874, 0.33728], b = 30.72065\n",
            "w_gradient = [ 0.02134705  0.          0.12018521 -0.14778489  0.13544431  0.01095957\n",
            " -0.00115726], b_gradient = -0.20787\n",
            "Iteration  5000 : cost = 18.69\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.77948, 0.00000, 0.52105, 2.35622, -1.87725, 0.25384, 0.33747], b = 30.80239\n",
            "w_gradient = [ 0.02111517  0.          0.10936143 -0.13130863  0.11925444  0.00869258\n",
            "  0.00029723], b_gradient = -0.12605\n",
            "Iteration  5500 : cost = 18.61\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.78988, 0.00000, 0.46898, 2.41818, -1.93330, 0.24999, 0.33705], b = 30.85196\n",
            "w_gradient = [ 0.0204254   0.          0.09902312 -0.11686013  0.10532168  0.00675063\n",
            "  0.00129794], b_gradient = -0.07643\n",
            "Iteration  6000 : cost = 18.57\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.79985, 0.00000, 0.42192, 2.47335, -1.98286, 0.24704, 0.33622], b = 30.88201\n",
            "w_gradient = [ 0.01941215  0.          0.08937149 -0.10411292  0.0932385   0.00513902\n",
            "  0.0019877 ], b_gradient = -0.04635\n",
            "Iteration  6500 : cost = 18.53\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.80925, 0.00000, 0.37950, 2.52253, -2.02678, 0.24481, 0.33510], b = 30.90024\n",
            "w_gradient = [ 0.01819633  0.          0.08048014 -0.0928253   0.08269626  0.0038272\n",
            "  0.00245646], b_gradient = -0.02810\n",
            "Iteration  7000 : cost = 18.50\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.81802, 0.00000, 0.34132, 2.56638, -2.06577, 0.24317, 0.33379], b = 30.91129\n",
            "w_gradient = [ 0.01687422  0.          0.07235697 -0.08280672  0.07345508  0.00277311\n",
            "  0.00276352], b_gradient = -0.01704\n",
            "Iteration  7500 : cost = 18.48\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.82612, 0.00000, 0.30702, 2.60551, -2.10041, 0.24200, 0.33236], b = 30.91799\n",
            "w_gradient = [ 0.01551721  0.          0.06497622 -0.07390042  0.06532424  0.0019342\n",
            "  0.00294983], b_gradient = -0.01033\n",
            "Iteration  8000 : cost = 18.47\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.83354, 0.00000, 0.27624, 2.64043, -2.13124, 0.24120, 0.33086], b = 30.92206\n",
            "w_gradient = [ 0.01417553  0.          0.0582955  -0.06597392  0.05814915  0.00127198\n",
            "  0.00304491], b_gradient = -0.00627\n",
            "Iteration  8500 : cost = 18.45\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.84030, 0.00000, 0.24863, 2.67162, -2.15869, 0.24070, 0.32932], b = 30.92452\n",
            "w_gradient = [ 0.0128827   0.          0.05226499 -0.05891341  0.05180246  0.00075342\n",
            "  0.00307079], b_gradient = -0.00380\n",
            "Iteration  9000 : cost = 18.44\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.84644, 0.00000, 0.22388, 2.69947, -2.18316, 0.24043, 0.32779], b = 30.92601\n",
            "w_gradient = [ 0.01165971  0.          0.04683251 -0.0526201   0.04617777  0.00035095\n",
            "  0.00304433], b_gradient = -0.00230\n",
            "Iteration  9500 : cost = 18.44\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.85198, 0.00000, 0.20171, 2.72435, -2.20497, 0.24034, 0.32629], b = 30.92692\n",
            "w_gradient = [ 1.05184067e-02  0.00000000e+00  4.19463761e-02 -4.70076163e-02\n",
            "  4.11852027e-02  4.18908268e-05  2.97867467e-03], b_gradient = -0.00140\n",
            "Iteration 10000 : cost = 18.43\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.85697, 0.00000, 0.18185, 2.74657, -2.22443, 0.24038, 0.32482], b = 30.92747\n",
            "w_gradient = [ 0.00946418  0.          0.03755697 -0.04200014  0.0367481  -0.00019222\n",
            "  0.00288421], b_gradient = -0.00085\n",
            "Iteration 10500 : cost = 18.42\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.86145, 0.00000, 0.16408, 2.76643, -2.24180, 0.24052, 0.32341], b = 30.92780\n",
            "w_gradient = [ 0.008498    0.          0.03361754 -0.03753085  0.03280058 -0.00036637\n",
            "  0.00276916], b_gradient = -0.00051\n",
            "Iteration 11000 : cost = 18.42\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.86548, 0.00000, 0.14817, 2.78418, -2.25730, 0.24074, 0.32205], b = 30.92800\n",
            "w_gradient = [ 0.00761787  0.          0.03008464 -0.03354073  0.02928564 -0.0004927\n",
            "  0.00264005], b_gradient = -0.00031\n",
            "Iteration 11500 : cost = 18.42\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.86908, 0.00000, 0.13394, 2.80004, -2.27114, 0.24101, 0.32077], b = 30.92813\n",
            "w_gradient = [ 0.00681993  0.          0.02691822 -0.02997752  0.02615366 -0.00058104\n",
            "  0.00250208], b_gradient = -0.00019\n",
            "Iteration 12000 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.87231, 0.00000, 0.12120, 2.81422, -2.28351, 0.24131, 0.31955], b = 30.92820\n",
            "w_gradient = [ 0.00609921  0.          0.02408165 -0.02679489  0.02336133 -0.00063931\n",
            "  0.00235931], b_gradient = -0.00011\n",
            "Iteration 12500 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.87519, 0.00000, 0.10981, 2.82689, -2.29455, 0.24164, 0.31841], b = 30.92825\n",
            "w_gradient = [ 0.00545014  0.          0.02154155 -0.0239517   0.02087062 -0.00067396\n",
            "  0.00221495], b_gradient = -0.00007\n",
            "Iteration 13000 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.87777, 0.00000, 0.09962, 2.83822, -2.30442, 0.24198, 0.31734], b = 30.92827\n",
            "w_gradient = [ 0.00486699  0.          0.01926764 -0.02141138  0.01864806 -0.00069015\n",
            "  0.00207148], b_gradient = -0.00004\n",
            "Iteration 13500 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.88007, 0.00000, 0.09050, 2.84834, -2.31324, 0.24233, 0.31634], b = 30.92829\n",
            "w_gradient = [ 0.00434403  0.          0.01723255 -0.0191414   0.01666415 -0.0006921\n",
            "  0.00193078], b_gradient = -0.00003\n",
            "Iteration 14000 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.88212, 0.00000, 0.08235, 2.85740, -2.32112, 0.24267, 0.31541], b = 30.92830\n",
            "w_gradient = [ 0.00387575  0.          0.01541156 -0.01711278  0.01489277 -0.00068318\n",
            "  0.00179427], b_gradient = -0.00002\n",
            "Iteration 14500 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.88395, 0.00000, 0.07506, 2.86549, -2.32816, 0.24301, 0.31454], b = 30.92831\n",
            "w_gradient = [ 0.00345692  0.          0.01378241 -0.0152997   0.01331079 -0.00066612\n",
            "  0.00166296], b_gradient = -0.00001\n",
            "Iteration 15000 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.88559, 0.00000, 0.06854, 2.87273, -2.33445, 0.24334, 0.31374], b = 30.92831\n",
            "w_gradient = [ 0.00308267  0.          0.01232507 -0.01367913  0.01189771 -0.00064311\n",
            "  0.00153758], b_gradient = -0.00001\n",
            "Iteration 15500 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.88704, 0.00000, 0.06271, 2.87920, -2.34008, 0.24365, 0.31300], b = 30.92831\n",
            "w_gradient = [ 0.00274849  0.          0.01102156 -0.01223055  0.01063528 -0.0006159\n",
            "  0.00141859], b_gradient = -0.00000\n",
            "Iteration 16000 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.88834, 0.00000, 0.05750, 2.88498, -2.34511, 0.24395, 0.31232], b = 30.92831\n",
            "w_gradient = [ 0.00245028  0.          0.00985574 -0.01093563  0.00950729 -0.00058588\n",
            "  0.00130625], b_gradient = -0.00000\n",
            "Iteration 16500 : cost = 18.41\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.88950, 0.00000, 0.05284, 2.89015, -2.34961, 0.24424, 0.31170], b = 30.92831\n",
            "w_gradient = [ 0.00218426  0.          0.00881312 -0.00977803  0.00849932 -0.00055417\n",
            "  0.00120066], b_gradient = -0.00000\n",
            "Iteration 17000 : cost = 18.40\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.89053, 0.00000, 0.04867, 2.89478, -2.35363, 0.24451, 0.31112], b = 30.92832\n",
            "w_gradient = [ 0.00194706  0.          0.00788074 -0.00874313  0.00759851 -0.00052163\n",
            "  0.00110181], b_gradient = -0.00000\n",
            "Iteration 17500 : cost = 18.40\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.89145, 0.00000, 0.04494, 2.89891, -2.35722, 0.24476, 0.31059], b = 30.92832\n",
            "w_gradient = [ 0.0017356   0.          0.00704697 -0.0078179   0.0067934  -0.00048893\n",
            "  0.00100959], b_gradient = -0.00000\n",
            "Iteration 18000 : cost = 18.40\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.89227, 0.00000, 0.04161, 2.90261, -2.36043, 0.24500, 0.31011], b = 30.92832\n",
            "w_gradient = [ 0.00154712  0.          0.00630139 -0.00699069  0.00607378 -0.0004566\n",
            "  0.00092383], b_gradient = -0.00000\n",
            "Iteration 18500 : cost = 18.40\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.89300, 0.00000, 0.03863, 2.90592, -2.36331, 0.24522, 0.30967], b = 30.92832\n",
            "w_gradient = [ 0.00137915  0.          0.00563471 -0.00625109  0.00543053 -0.00042503\n",
            "  0.00084428], b_gradient = -0.00000\n",
            "Iteration 19000 : cost = 18.40\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.89365, 0.00000, 0.03596, 2.90887, -2.36588, 0.24542, 0.30926], b = 30.92832\n",
            "w_gradient = [ 0.00122946  0.          0.00503857 -0.00558982  0.00485552 -0.0003945\n",
            "  0.00077068], b_gradient = -0.00000\n",
            "Iteration 19500 : cost = 18.40\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "w = [-0.89423, 0.00000, 0.03358, 2.91151, -2.36817, 0.24561, 0.30890], b = 30.92832\n",
            "w_gradient = [ 0.00109633  0.          0.00450652 -0.00499968  0.00434245 -0.00036529\n",
            "  0.00070286], b_gradient = -0.00000\n",
            "Iteration 19999 : cost = 18.40\n",
            "----------------------------------------------------\n",
            "MAPE on male validation dataset: 10.71%\n",
            "MAPE on female validation dataset: 11.38%\n",
            "Predictions saved to lab1_advanced.csv.\n"
          ]
        }
      ]
    }
  ]
}